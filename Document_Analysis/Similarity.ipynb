{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Similarity.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanimuranaomichi/Information_System_Analysis/blob/master/Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aVVn6hB9Zei"
      },
      "source": [
        "# Preprocessing and calculate similarity\n",
        "\n",
        "ã“ã®ãƒãƒ¼ãƒˆã®ç›®æ¨™ã¯è‡ªåŠ›ã§æ–‡æ›¸ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã“ã¨  \n",
        "æœ€çµ‚çš„ã«Wikipediaã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦å›½ã®é¡ä¼¼åº¦ã‚’æ¸¬ã‚Š  \n",
        "æ—¥æœ¬ã¨ä¼¼ã¦ã„ã‚‹å›½ã‚’æ¢ã™"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5-315WVmN64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a27f9a80-c6f7-477b-d262-328e01b1557a"
      },
      "source": [
        "# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!pip install nltk\n",
        "!pip install gensim"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8iK7xBDTmtf"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBUvXHj7mRls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "318f042c-76b4-4cdc-b9fa-5e94c6fad959"
      },
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpg2beFhKA2R"
      },
      "source": [
        "## 1. Calculate similarity\n",
        "\n",
        "ä»¥ä¸‹ã®ä¸‰ã¤ã®æ–‡ã‚’è€ƒãˆã‚‹  \n",
        "Doc A : \"I like apples and a strawberries. I will buy an apple tomorrow. \"  \n",
        "Doc B : \"I bought some apples and strawberries. I will eat an apple tomorrow.\"  \n",
        "Doc C : \"I play basketball every day. I like Michael Jordan.\"  \n",
        "\n",
        "Doc Aã¨Doc Bã¯ä¼¼ã¦ã„ãã†ã ãŒã€Doc Cã¯Doc Aã¨ã‚‚Doc Bã¨ã‚‚ä¼¼ã¦ã„ãªã•ãã†  \n",
        "ã“ã‚Œã‚’é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã§ç¢ºã‹ã‚ã‚‹\n",
        "\n",
        "é¡ä¼¼åº¦ã®è¨ˆç®—ã®ä»•æ–¹ã¯ã„ãã¤ã‹ã‚ã‚‹\n",
        "\n",
        "- é›†åˆãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦\n",
        "  - Jaccardä¿‚æ•°\n",
        "  - Diceä¿‚æ•°\n",
        "  - Simpsonä¿‚æ•°\n",
        "- ãƒ™ã‚¯ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦\n",
        "  - ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢\n",
        "  - ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1rpPCaUeOWE"
      },
      "source": [
        "### é›†åˆãƒ™ãƒ¼ã‚¹\n",
        "\n",
        "æ–‡æ›¸ã‚’å˜èªã®é›†åˆã«å¤‰æ›ã™ã‚‹  \n",
        "é›†åˆãªã®ã§é‡è¤‡ã—ãŸå˜èªã¯å‰Šé™¤ã™ã‚‹  \n",
        "å‰å‡¦ç†ã¯ä»Šå›ã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹   \n",
        "\n",
        "Doc A : \"I like apples and a strawberries. I will buy an apple tomorrow. \"  \n",
        "Doc B : \"I bought some apples and strawberries. I will eat an apple tomorrow.\"  \n",
        "Doc C : \"I play basketball every day. I like Michael Jordan.\"  \n",
        "â†“    \n",
        "Set A : {'a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'}  \n",
        "Set B : {'an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'}  \n",
        "Set C : {'basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'}  \n",
        "\n",
        "ã“ã®é›†åˆãŒæ–‡æ›¸ã®ç‰¹å¾´ã‚’è¡¨ã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‹  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uts5Ns2eKDaW"
      },
      "source": [
        "#### Jaccardä¿‚æ•°\n",
        "Jaccardä¿‚æ•°ã¯äºŒã¤ã®é›†åˆA,Bã«å¯¾ã—ã¦å®šç¾©ã•ã‚Œã‚‹é¡ä¼¼åº¦ã§ã‚ã‚‹  \n",
        "è¨ˆç®—å¼ã¯ä»¥ä¸‹ã®é€šã‚Š\n",
        "\n",
        "\\begin{equation}\n",
        "J(A,B)=\\dfrac{|A\\cap B|}{|A \\cup B|}\n",
        "\\end{equation}\n",
        "\n",
        "å…±é€šéƒ¨åˆ†ã®å‰²åˆãŒå¤§ãã‘ã‚Œã°ãã®äºŒã¤ã®æ–‡æ›¸ã¯ä¼¼ã¦ã„ã‚‹ã¨è€ƒãˆã‚‹"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqr10Mw-K5UQ"
      },
      "source": [
        "def jaccard_similarity(set_a,set_b):\n",
        "  # ç©é›†åˆã®è¦ç´ æ•°ã‚’è¨ˆç®—\n",
        "  num_intersection = len(set.intersection(set_a, set_b))\n",
        "  # å’Œé›†åˆã®è¦ç´ æ•°ã‚’è¨ˆç®—\n",
        "  num_union = len(set.union(set_a, set_b))\n",
        "  #Jaccardä¿‚æ•°ã‚’ç®—å‡ºã€€ç©ºé›†åˆã®æ™‚ã¯1ã‚’å‡ºåŠ›\n",
        "  try:\n",
        "      return float(num_intersection) / num_union\n",
        "  except ZeroDivisionError:\n",
        "      return 1.0 "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZSFY5urK8eT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7084507-dea7-4b0d-cb47-78cdc372bc6f"
      },
      "source": [
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "\n",
        "print(\"jaccard(a, b) = \", jaccard_similarity(set_a, set_b)) #Jaccardä¿‚æ•°ã‚’è¨ˆç®—\n",
        "print(\"jaccard(a, c) = \", jaccard_similarity(set_a, set_c))\n",
        "print(\"jaccard(b, c) = \", jaccard_similarity(set_b, set_c))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jaccard(a, b) =  0.5714285714285714\n",
            "jaccard(a, c) =  0.11764705882352941\n",
            "jaccard(b, c) =  0.05555555555555555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXBk5SiTMmGf"
      },
      "source": [
        "\n",
        "nltkã§å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹  \n",
        "å®šç¾©ã¨åŒã˜ã‚ˆã†ã«è¨ˆç®—ã‚’è¡Œã†ã®ã§ã€å…¥åŠ›ã¯é›†åˆ  \n",
        "è·é›¢ã«ãªã£ã¦ã„ã‚‹ã¨ã“ã‚ã«ã¯æ³¨æ„ãŒå¿…è¦"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZQ9q8mFLJTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "293f3b47-fd84-4372-d6a6-dd044a03ef67"
      },
      "source": [
        "from nltk.metrics import jaccard_distance\n",
        "\n",
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "\n",
        "# Jaccardè·é›¢ã«ãªã£ã¦ã„ã‚‹ã®ã§ã€é¡ä¼¼åº¦ã«å¤‰æ›ã™ã‚‹ã¨ãã¯1ã‹ã‚‰å¼•ã\n",
        "print(\"jaccard(a, b) = \", 1 - jaccard_distance(set_a, set_b))\n",
        "print(\"jaccard(a, c) = \", 1 - jaccard_distance(set_a, set_c))\n",
        "print(\"jaccard(b, c) = \", 1 - jaccard_distance(set_b, set_c))\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jaccard(a, b) =  0.5714285714285714\n",
            "jaccard(a, c) =  0.11764705882352944\n",
            "jaccard(b, c) =  0.05555555555555558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lyCVTfePwB2"
      },
      "source": [
        "#### SÃ¸rensen-Diceä¿‚æ•°\n",
        "\n",
        "Jaccardä¿‚æ•°ã§ã¯åˆ†æ¯ã¯ã®å’Œé›†åˆã§ã‚ã£ãŸãŸã‚  \n",
        "ç‰‡æ–¹ã®é›†åˆãŒã¨ã¦ã‚‚å¤§ãã„ã¨å…±é€šéƒ¨åˆ†ãŒå¤§ããã¦ã‚‚ä¿‚æ•°ã®å€¤ãŒå°ã•ããªã£ã¦ã—ã¾ã†ã¨ã„ã†å•é¡ŒãŒã‚ã‚‹  \n",
        "SÃ¸rensen-Diceä¿‚æ•°ã§ã¯ã€åˆ†æ¯ã‚’äºŒã¤ã®é›†åˆã®å¤§ãã•ã®å¹³å‡ã‚’ã¨ã‚‹ã“ã¨ã§ã€ãã®å½±éŸ¿ã‚’ç·©å’Œã—ã¦ã„ã‚‹  \n",
        "\n",
        "$\n",
        "DSC(A,B) = \\dfrac{|A\\cap B|}{\\dfrac{|A| + |B|}{2}} = \\dfrac{2|A\\cap B|}{|A| + |B|}\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M0RikFPR3Qr"
      },
      "source": [
        "def dice_similarity(set_a, set_b):\n",
        "  num_intersection =  len(set.intersection(set_a, set_b))\n",
        "  sum_nums = len(set_a) + len(set_b)\n",
        "  try:\n",
        "    return 2 * num_intersection / sum_nums\n",
        "  except ZeroDivisionError:\n",
        "    return 1.0 "
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZQFbXlESPWl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81957556-1a3b-40c7-cb89-11e4648b59d0"
      },
      "source": [
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "\n",
        "print(\"dice(a, b) = \", dice_similarity(set_a, set_b))\n",
        "print(\"dice(a, c) = \", dice_similarity(set_a, set_c))\n",
        "print(\"dice(b, c) = \", dice_similarity(set_b, set_c))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dice(a, b) =  0.7272727272727273\n",
            "dice(a, c) =  0.21052631578947367\n",
            "dice(b, c) =  0.10526315789473684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxtliI-HPwRE"
      },
      "source": [
        "#### Szymkiewicz-Simpsonä¿‚æ•°\n",
        "\n",
        "å·®é›†åˆã®è¦ç´ æ•°ã®å½±éŸ¿ã‚’æ¥µé™ã¾ã§æŠ‘ãˆãŸã®ãŒSzymkiewicz-Simpsonä¿‚æ•°    \n",
        "$\n",
        "overlap(ğ´,ğµ) = \\dfrac{|A\\cap B|}{\\min(|A|, |B|)}\n",
        "$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgGJ5GhmUduH"
      },
      "source": [
        "def simpson_similarity(list_a, list_b):\n",
        "  num_intersection = len(set.intersection(set(list_a), set(list_b)))\n",
        "  min_num = min(len(set(list_a)), len(set(list_b)))\n",
        "  try:\n",
        "    return num_intersection / min_num\n",
        "  except ZeroDivisionError:\n",
        "    if num_intersection == 0:\n",
        "      return 1.0\n",
        "    else:\n",
        "      return 0"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_1Gjy4IV9eo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c0bf9b-8df7-44cb-bb7c-7232fc18a691"
      },
      "source": [
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "\n",
        "print(\"simpson(a, b) = \", simpson_similarity(set_a, set_b)) \n",
        "print(\"simpson(a, c) = \", simpson_similarity(set_a, set_c)) \n",
        "print(\"simpson(b, c) = \", simpson_similarity(set_b, set_c)) "
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "simpson(a, b) =  0.7272727272727273\n",
            "simpson(a, c) =  0.25\n",
            "simpson(b, c) =  0.125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-T32gmnZIyt"
      },
      "source": [
        "#### Exercise 1\n",
        "è‰²ã€…ãªé›†åˆã‚’ä½œã£ã¦é›†åˆãƒ™ãƒ¼ã‚¹æ‰‹æ³•ã®æ¯”è¼ƒã‚’ã—ã‚ˆã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJwJpZpdYtz9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace85397-8f72-4693-e383-8fa02cf8d6d2"
      },
      "source": [
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "set_d = set() # å¤§ãã‚ã®é›†åˆã‚’ä½œã£ã¦è©¦ã—ã¦ã¿ã‚ˆã†\n",
        "\n",
        "print(\"jaccard similarity:\")\n",
        "print(jaccard_similarity(set_d, set_a))\n",
        "print(jaccard_similarity(set_d, set_b))\n",
        "print(jaccard_similarity(set_d, set_c))\n",
        "\n",
        "print(\"dice similarity:\")\n",
        "print(dice_similarity(set_d, set_a))\n",
        "print(dice_similarity(set_d, set_b))\n",
        "print(dice_similarity(set_d, set_c))\n",
        "\n",
        "print(\"simpson similarity:\")\n",
        "print(simpson_similarity(set_d, set_a))\n",
        "print(simpson_similarity(set_d, set_b))\n",
        "print(simpson_similarity(set_d, set_c))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jaccard similarity:\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "dice similarity:\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "simpson similarity:\n",
            "1.0\n",
            "1.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-Hm34tgXCbh"
      },
      "source": [
        "### ãƒ™ã‚¯ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ \n",
        "\n",
        "\n",
        "æ–‡æ›¸ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦è¡¨ç¾ã—é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹  \n",
        "ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã®æ‰‹æ³•ã¯è‰²ã€…ã‚ã‚‹ãŒä»Šå›ã¯BoW(Bag of Words)ã§èª¬æ˜ã™ã‚‹  \n",
        "\n",
        "BoWã¯æ–‡ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ç¾ã™ã‚‹æ–¹æ³•ã®ä¸€ã¤  \n",
        "æƒ³å®šã—ã¦ã„ã‚‹å˜èªã®ç·æ•°ã‚’Nã¨ã™ã‚‹ã¨ã€å„æ¬¡å…ƒãŒå„å˜èªã«å¯¾å¿œã™ã‚‹Næ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’è€ƒãˆã‚‹  \n",
        "å„æ¬¡å…ƒã®å€¤ã¯ãã®å˜èªãŒæ–‡æ›¸ä¸­ã§å‡ºãŸå›æ•°\n",
        "\n",
        "ä¾‹ï¼‰  \n",
        "Doc A : \"I like apples and a strawberries. I will buy an apple tomorrow. \"  \n",
        "Doc B : \"I bought some apples and strawberries. I will eat an apple tomorrow.\"  \n",
        "Doc C : \"I play basketball every day. I like Michael Jordan.\"  \n",
        "â†“  \n",
        "å…¨å˜èªã¯19å€‹ã§ã€å„æ¬¡å…ƒã®å€¤ã¯ä»¥ä¸‹ã®å˜èªã®å€‹æ•°ã«å¯¾å¿œã™ã‚‹BoWã‚’è€ƒãˆã‚‹  \n",
        "['an', 'and', 'apple', 'apples', 'basketball', 'bought', 'buy', 'day', 'eat', 'every', 'i', 'jordan', 'like', 'michael', 'play', 'some', 'strawberries', 'tomorrow', 'will']  \n",
        "â†“  \n",
        "BoW A : [1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 1]  \n",
        "BoW B : [1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1]  \n",
        "BoW C : [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0]  \n",
        "\n",
        "ã“ã®ãƒ™ã‚¯ãƒˆãƒ«ãŒæ–‡æ›¸ã®ç‰¹å¾´ã‚’è¡¨ã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQOjHTfCeXjs"
      },
      "source": [
        "\n",
        "#### ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢\n",
        "\n",
        "å„æ–‡æ›¸ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ã™ã“ã¨ãŒå‡ºæ¥ãŸã®ã§  \n",
        "ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ãŒè¨ˆç®—ã§ãã‚‹  \n",
        "ã“ã®è·é›¢ãŒå°ã•ã‘ã‚Œã°ä¼¼ã¦ã„ã‚‹ã¨è€ƒãˆã‚‹ã“ã¨ãŒå‡ºæ¥ã‚‹\n",
        "\n",
        "\\begin{equation}\n",
        "d(v_1,v_2) =(\\sum_{i=1}^n (v_{1i}-v_{2i})^2)^{\\frac{1}{2}}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtEzuSDKZbpV"
      },
      "source": [
        "def euclidean_distance(list_a, list_b):\n",
        "  diff_vec = np.array(list_a) - np.array(list_b)\n",
        "  return np.linalg.norm(diff_vec)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Sr_ZYj7azLV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014dd812-4b58-4ec8-b7e5-8ce31c7ac483"
      },
      "source": [
        "bow_a = [1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 1]  \n",
        "bow_b = [1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1]  \n",
        "bow_c = [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0]  \n",
        "\n",
        "print(\"euclidean_distance(bow_a, bow_b) = \",euclidean_distance(bow_a, bow_b))\n",
        "print(\"euclidean_distance(bow_a, bow_c) = \",euclidean_distance(bow_a, bow_c))\n",
        "print(\"euclidean_distance(bow_b, bow_c) = \",euclidean_distance(bow_b, bow_c))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "euclidean_distance(bow_a, bow_b) =  2.23606797749979\n",
            "euclidean_distance(bow_a, bow_c) =  3.7416573867739413\n",
            "euclidean_distance(bow_b, bow_c) =  4.123105625617661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfKcQd5SiBb-"
      },
      "source": [
        "#### ãƒŸãƒ³ã‚³ãƒ•ã‚¹ã‚­ãƒ¼è·é›¢\n",
        "\n",
        "ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã‚’ä¸€èˆ¬åŒ–ã—ãŸè·é›¢\n",
        "pã®å€¤ã‚’å¤‰ãˆã‚‹ã“ã¨ã§è‰²ã€…ãªè·é›¢ã‚’è¡¨ç¾ã§ãã‚‹  \n",
        "\n",
        "\\begin{equation}\n",
        "d(v_1,v_2) = (\\sum_{i=1}^n |v_{1i}-v_{2i}|^p)^{\\frac{1}{p}}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx46KLSNhjI2"
      },
      "source": [
        "#### Exercise 2\n",
        "ãƒŸãƒ³ã‚³ãƒ•ã‚¹ã‚­ãƒ¼è·é›¢ã‚’è¨ˆç®—ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ›¸ã„ã¦  \n",
        "p=1,2,3ã§è·é›¢ã‚’è¨ˆç®—ã—ã¦ã¿ã‚ˆã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6UMFh58bexH"
      },
      "source": [
        "# np.linalg.normã«ã¤ã„ã¦èª¿ã¹ã‚ˆã†\n",
        "def minkowski_distance(list_a, list_b, p):\n",
        "  diff_vec = np.array(list_a) - np.array(list_b)\n",
        "  return np.linalg.norm(diff_vec, ord=p)\n",
        "  "
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKMIMbyJc8eH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2022bd7e-538a-4cd3-cb2c-e9936bd60fd2"
      },
      "source": [
        "# p=1\n",
        "print(minkowski_distance(bow_a, bow_b, 1))\n",
        "print(minkowski_distance(bow_a, bow_c, 1))\n",
        "print(minkowski_distance(bow_b, bow_c, 1))\n",
        "\n",
        "# p=2\n",
        "print(minkowski_distance(bow_a, bow_b, 2))\n",
        "print(minkowski_distance(bow_a, bow_c, 2))\n",
        "print(minkowski_distance(bow_b, bow_c, 2))\n",
        "\n",
        "# p=3\n",
        "print(minkowski_distance(bow_a, bow_b, 3))\n",
        "print(minkowski_distance(bow_a, bow_c, 3))\n",
        "print(minkowski_distance(bow_b, bow_c, 3))\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.0\n",
            "14.0\n",
            "17.0\n",
            "2.23606797749979\n",
            "3.7416573867739413\n",
            "4.123105625617661\n",
            "1.7099759466766968\n",
            "2.4101422641752297\n",
            "2.571281590658235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIyabGRTKGwA"
      },
      "source": [
        "#### ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦\n",
        "\n",
        "ãƒ™ã‚¯ãƒˆãƒ«ã®ãªã™è§’ã«ç€ç›®ã—ã¦é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹  \n",
        "\n",
        "\\begin{equation}\n",
        "similarity(A, B)=cos(\\theta)=\\dfrac{\\sum_{i=1}^n A_iB_i}{{\\sqrt A}{\\sqrt B}}\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fWy619amlwp"
      },
      "source": [
        "#### Exercise 3\n",
        "ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ›¸ã„ã¦è¨ˆç®—ã—ã‚ˆã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDhwqTN5yJGQ"
      },
      "source": [
        "# numpy.array ã«ã¤ã„ã¦èª¿ã¹ã‚ˆã†\n",
        "def cosine_similarity(list_a, list_b):\n",
        "  # ã‚ã¨ã§æ¶ˆã™\n",
        "  inner_prod = np.array(list_a).dot(np.array(list_b))\n",
        "  norm_a = np.linalg.norm(list_a)\n",
        "  norm_b = np.linalg.norm(list_b)\n",
        "  try:\n",
        "      return inner_prod / (norm_a*norm_b)\n",
        "  except ZeroDivisionError:\n",
        "      return 1.0"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sew3u-YezRrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea2c18a9-a5a9-4d5b-f1a2-4bf24c92b602"
      },
      "source": [
        "bow_a = [1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 1]\n",
        "bow_b = [1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1]\n",
        "bow_c = [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0]\n",
        "\n",
        "print(\"cosine_similarity(bow_a, bow_b) = \",cosine_similarity(bow_a, bow_b))\n",
        "print(\"cosine_similarity(bow_a, bow_c) = \",cosine_similarity(bow_a, bow_c))\n",
        "print(\"cosine_similarity(bow_b, bow_c) = \",cosine_similarity(bow_b, bow_c))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cosine_similarity(bow_a, bow_b) =  0.8153742483272114\n",
            "cosine_similarity(bow_a, bow_c) =  0.41812100500354543\n",
            "cosine_similarity(bow_b, bow_c) =  0.3223291856101521\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao0jywXqKIFS"
      },
      "source": [
        "### é›†åˆãƒ™ãƒ¼ã‚¹ã¨ãƒ™ã‚¯ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ã®æ¯”è¼ƒ\n",
        "\n",
        "é›†åˆæ¼”ç®—ã®æ–¹ã¯ä¸€ã¤ä¸€ã¤ã®æ–‡æ›¸ãŒå°ã•ã„ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦æ€§èƒ½ãŒé«˜ã„  \n",
        "æ–‡æ›¸ãŒã‚ã‚‹ç¨‹åº¦å¤§ãããªã‚‹ã¨ãƒ™ã‚¯ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ã®æ–¹ãŒæœ‰ç”¨ã«ãªã‚‹  \n",
        "ãã®ä»£ã‚ã‚Šã€èªå½™é›†åˆãŒå¤§ãããªã‚Šè¨ˆç®—é‡ãŒå¤§ãããªã£ã¦ã—ã¾ã†\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1IXMwIyhU-6"
      },
      "source": [
        "### Exercise 4\n",
        "çŸ­ã„æ–‡ç« ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨é•·ã„æ–‡ç« ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‡ªåˆ†ã§ä½œã‚Š    \n",
        "Jaccardä¿‚æ•°ã¨ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã¦æ¯”è¼ƒã—ã¦ã¿ã‚ˆã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpkcXVDBhZg8"
      },
      "source": [
        "short_docs = []\n",
        "long_docs = []"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuXSHk01KLza"
      },
      "source": [
        "## 2. Preprocessing\n",
        "\n",
        "é›†åˆé–“ã®å…±é€šéƒ¨åˆ†ã‚„ãƒ™ã‚¯ãƒˆãƒ«é–“ã®è·é›¢ã‚„è§’åº¦ã§é¡ä¼¼åº¦ã‚’æ¸¬ã‚‹ã“ã¨ãŒå‡ºæ¥ãŸ  \n",
        "é›†åˆã‚„ãƒ™ã‚¯ãƒˆãƒ«ãŒæ–‡æ›¸ã®ç‰¹å¾´ã‚’ä¸Šæ‰‹ãè¡¨ã›ã¦ã„ãªã„ã¨é¡ä¼¼åº¦ãŒä¸Šæ‰‹ãæ¸¬ã‚Œãªã„  \n",
        "æ–‡æ›¸ã‹ã‚‰ã©ã®ã‚ˆã†ã«é›†åˆã‚„ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œã‚‹ã‹ãŒã¨ã¦ã‚‚å¤§äº‹  \n",
        " \n",
        "é©åˆ‡ãªå‰å‡¦ç†ã‚’è¡Œã†ã“ã¨ã§ç‰¹å¾´ã‚’æ‰ãˆãŸé¡ä¼¼åº¦ã‚’æ¸¬ã‚Œã‚‹ã‚ˆã†ã«ãªã‚‹    \n",
        "å¾ŒåŠã¯ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«çµã£ã¦ç·´ç¿’ã—ã¦ã„ã  \n",
        "\n",
        "1. Clearning\n",
        "2. Tokenize\n",
        "3. Stemming\n",
        "4. Remove stop words\n",
        "5. Vectorize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4ispTSLKM-z"
      },
      "source": [
        "### 2-1. Clearning\n",
        "\n",
        "ä¸Šã®ä¾‹ã§ã¯ç¶ºéº—ãªæ–‡ç« ã°ã‹ã‚Šæ‰±ã£ã¦ã„ãŸãŒã€å®Ÿéš›ã¯ã‚‚ã£ã¨æ±šã„   \n",
        "Webã‹ã‚‰å–ã£ã¦ããŸãƒ‡ãƒ¼ã‚¿ã ã¨htmlã‚¿ã‚°ãŒæ®‹ã£ã¦ã„ãŸã‚Šã€å¤‰ãªè¨˜å·ãŒå…¥ã£ã¦ã„ãŸã‚Šã™ã‚‹  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM-e5AZT316r"
      },
      "source": [
        "documents=[\"I like apples and a strawberries. I will buy an apple tomorrow @Fresco.\",\n",
        "           \"I bought some apples and strawberries. I will eat an apple <b>tomorrow.</b>\",\n",
        "           \"I play basketball every day. I like Michael Jordan (born February 17, 1963).\"]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh56eU_levzv"
      },
      "source": [
        "ä»Šã¯ä¸‰ã¤ãªã®ã§æ‰‹å‹•ã§æ¶ˆã›ã‚‹ãŒ  \n",
        "å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†ã¨ãã«ã¯è‡ªå‹•ã§ç¶ºéº—ã«ã§ããªã„ã¨ã„ã‘ãªã„  \n",
        "ç¶ºéº—ã«ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ä½œã‚‹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06coot4PnVgS"
      },
      "source": [
        "#### Exercise 5\n",
        "\n",
        "æ­£è¦è¡¨ç¾ã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¶ºéº—ã«ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ›¸ã“ã†\n",
        "\n",
        "å‚è€ƒ: æ­£è¦è¡¨ç¾ (https://uxmilk.jp/41416)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt3NL8Ux5Q4E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "454a0390-1ccc-4ee3-8616-0af316234673"
      },
      "source": [
        "import re\n",
        "\n",
        "def cleaning_text(text):\n",
        "    # @ã®å‰Šé™¤\n",
        "    pattern1 = '@'\n",
        "    text = re.sub(pattern1, '', text)    \n",
        "    # <b>ã‚¿ã‚°ã®å‰Šé™¤\n",
        "    pattern2 = # \n",
        "    text = re.sub(pattern2, '', text)    \n",
        "    # ()å†…ã‚’å‰Šé™¤\n",
        "    pattern3 = #\n",
        "    text = re.sub(pattern3, '', text)\n",
        "    return text\n",
        "  \n",
        "\n",
        "for text in documents:\n",
        "    print(cleaning_text(text))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-74-e3f9fcc5da52>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    pattern2 = #\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcgcKWkRflR5"
      },
      "source": [
        "#### Option 1\n",
        "\n",
        "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¶ºéº—ã«ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ã¿ã‚ˆã†\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcsdHgV-h26y"
      },
      "source": [
        "text = '<p><b>Natural language processing</b> (<b>NLP</b>) is a subfield of <a href=\"/wiki/Computer_science\" title=\"Computer science\">computer science</a>, <a href=\"/wiki/Information_engineering_(field)\" title=\"Information engineering (field)\">information engineering</a>, and <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a> concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of <a href=\"/wiki/Natural_language\" title=\"Natural language\">natural language</a> data.</p>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1doxCMAGr0_O"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edJ80nTbQgLi"
      },
      "source": [
        "\n",
        "### 2-2. Tokenize\n",
        "\n",
        "ã¾ã æ–‡å­—åˆ—ã®ã¾ã¾ãªã®ã§ã€å˜èªã”ã¨ã«åŒºåˆ‡ã‚‹  \n",
        "è‹±èªã ã¨ç©ºç™½åŒºåˆ‡ã‚Šã§ã‚ˆã„ãŒæ—¥æœ¬èªã ã¨å°‘ã—é¢å€’  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGbqTvkyYl2S"
      },
      "source": [
        "def tokenize_text(text):\n",
        "  text = re.sub('[.,]', '', text)\n",
        "  return text.split()\n",
        "\n",
        "for text in documents:\n",
        "  text = cleaning_text(text)\n",
        "  print(tokenize_text(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K97zCvZBQik9"
      },
      "source": [
        "### 2-3. Stemming, Lemmatize\n",
        "\n",
        "åŒã˜æ„å‘³ã®å˜èªã§ã‚‚ç•°ãªã‚‹å½¢ã‚’ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‚‹  \n",
        "ãã‚Œã‚‰ã‚’åˆ¥ã®å˜èªã¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ã®ã¯ä¸è‡ªç„¶  \n",
        "å°æ–‡å­—ã«å¤‰æ›ã—ãŸå¾Œ  \n",
        "Stemmingã‚„Lemmatizeã¨ã„ã†å‡¦ç†ã§åŒã˜å½¢ã«ã™ã‚‹  \n",
        "ä»Šå›ã¯Lemmatizeã®ã¿"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7R55rKxZtAz"
      },
      "source": [
        "from nltk.corpus import wordnet as wn #lemmatizeé–¢æ•°ã®ãŸã‚ã®import\n",
        "\n",
        "def lemmatize_word(word):\n",
        "    # make words lower  example: Python =>python\n",
        "    word=word.lower()\n",
        "    \n",
        "    # lemmatize  example: cooked=>cook\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "      return lemma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0sEVvmwaCqA"
      },
      "source": [
        "for text in documents:\n",
        "  text = cleaning_text(text)\n",
        "  tokens = tokenize_text(text)\n",
        "  print([lemmatize_word(word) for word in tokens])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOPIFqvjfxEg"
      },
      "source": [
        "strawberriesâ†’strawberryã®ã‚ˆã†ã«èªã‚’æ¨™æº–å½¢ã«å¤‰æ›å‡ºæ¥ãŸ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpUxnB9kQlKv"
      },
      "source": [
        "### 2-4. Remove stop words\n",
        "\n",
        "a, theãªã©ã®æ–‡ç« ã«å¯„ã‚‰ãšä¸€èˆ¬çš„ã«ä½¿ã‚ã‚Œã‚‹å† è©ã€ä»£åè©ã€å‰ç½®è©ãªã©ã‚’ä½¿ã£ã¦ã‚‚æ„å‘³ãŒãªã„  \n",
        "ãã‚Œã‚‰ã®å˜èªã¯stop wordã¨å‘¼ã°ã‚Œã‚‹  \n",
        "nltkã«ã¯å°‚é–€å®¶ãŒå®šç¾©ã—ãŸstop wordã®ãƒªã‚¹ãƒˆãŒã‚ã‚‹ã®ã§ãã‚Œã‚’ä½¿ã†  \n",
        "å¿…è¦ã«å¿œã˜ã¦stop wordã¯è‡ªåˆ†ã§ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹ã¹ã  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbSLDay-a36N"
      },
      "source": [
        "#1 nltkã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ\n",
        "en_stop = nltk.corpus.stopwords.words('english')\n",
        "print(en_stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX8eIyxfbo6q"
      },
      "source": [
        "def remove_stopwords(word, stopwordset):\n",
        "  if word in stopwordset:\n",
        "    return None\n",
        "  else:\n",
        "    return word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WckIX0ThbQMy"
      },
      "source": [
        "for text in documents:\n",
        "  text = cleaning_text(text)\n",
        "  tokens = tokenize_text(text)\n",
        "  tokens = [lemmatize_word(word) for word in tokens]\n",
        "  print([remove_stopwords(word, en_stop) for word in tokens])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4kNHhr5f8Vt"
      },
      "source": [
        "ä»Šå›ã¯ã“ã‚Œã ã‘ã§çµ‚ã‚ã‚Šã«ã™ã‚‹ãŒå˜èªã®å‰Šé™¤ã¯ã‹ãªã‚Šé‡è¦  \n",
        "å‡ºç¾é »åº¦ãŒæ¥µç«¯ã«ä½ã„å˜èªã‚’å‰Šé™¤ã—ãŸã‚Šã€å‹•è©ã¨åè©ã«é™å®šã™ã‚‹ãªã©è‰²ã€…ã‚ã‚‹"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgjfOITFxwx_"
      },
      "source": [
        "def preprocessing_text(text):\n",
        "  text = cleaning_text(text)\n",
        "  tokens = tokenize_text(text)\n",
        "  tokens = [lemmatize_word(word) for word in tokens]\n",
        "  tokens = [remove_stopwords(word, en_stop) for word in tokens]\n",
        "  tokens = [word for word in tokens if word is not None]\n",
        "  return tokens\n",
        "\n",
        "\n",
        "preprocessed_docs = [preprocessing_text(text) for text in documents]\n",
        "preprocessed_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCd6YpYbKJSJ"
      },
      "source": [
        "### 2-5. Vectorize\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmrBtgPxcwX0"
      },
      "source": [
        "#### BoW(Bag of Words)\n",
        "\n",
        "\n",
        "ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªã®å‡ºç¾å›æ•°ã®ãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ã—ãŸã‚‚ã®  \n",
        "äººæ‰‹ã§å˜èªã‚’æ•°ãˆãŸã‚Šã™ã‚‹ã®ã¯ä¸å¯èƒ½ãªã®ã§ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§å‡¦ç†ã‚’å®Œçµã—ã¦ã—ã¾ãŠã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S69MsPjHcvut"
      },
      "source": [
        "def bow_vectorizer(docs):\n",
        "  word2id = {}\n",
        "  for doc in docs:\n",
        "    for w in doc:\n",
        "      if w not in word2id:\n",
        "        word2id[w] = len(word2id)\n",
        "        \n",
        "  result_list = []\n",
        "  for doc in docs:\n",
        "    doc_vec = [0] * len(word2id)\n",
        "    for w in doc:\n",
        "      doc_vec[word2id[w]] += 1\n",
        "    result_list.append(doc_vec)\n",
        "  return result_list, word2id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq3TpP7KO-XP"
      },
      "source": [
        "bow_vec, word2id = bow_vectorizer(preprocessed_docs)\n",
        "print(bow_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udTTbKv8oFpG"
      },
      "source": [
        "word2id.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e8h1SMTcyD5"
      },
      "source": [
        "### TF-IDF(Term Frequency - Inverse Document Frequency)\n",
        "\n",
        "BoWã§ã¯å„å˜èªã®é‡ã¿ãŒåŒã˜ã ã£ãŸãŒã€å˜èªã«ã‚ˆã£ã¦é‡è¦åº¦ã¯å¤‰ã‚ã‚‹  \n",
        "å˜èªã®é‡è¦åº¦ã‚’è€ƒæ…®ã—ãŸã®ãŒTF-IDF  \n",
        "\n",
        "TF(t, d) = ã‚ã‚‹å˜èª(t)ã®ã‚ã‚‹æ–‡æ›¸(d)ã«ãŠã‘ã‚‹å‡ºç¾é »åº¦  \n",
        "IDF(t) = ã‚ã‚‹å˜èª(t)ãŒå…¨æ–‡æ›¸é›†åˆ(D)ä¸­ã«ã©ã‚Œã ã‘ã®æ–‡æ›¸ã§å‡ºç¾ã—ãŸã‹ã®é€†æ•°  \n",
        "\n",
        "TF-IDF(t,d) = TF(t, d) * IDF(t)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiKn2p7Bc0bN"
      },
      "source": [
        "def tfidf_vectorizer(docs):\n",
        "  def tf(word2id, doc):\n",
        "    term_counts = np.zeros(len(word2id))\n",
        "    for term in word2id.keys():\n",
        "      term_counts[word2id[term]] = doc.count(term)\n",
        "    tf_values = list(map(lambda x: x/sum(term_counts), term_counts))\n",
        "    return tf_values\n",
        "  \n",
        "  def idf(word2id, docs):\n",
        "    idf = np.zeros(len(word2id))\n",
        "    for term in word2id.keys():\n",
        "      idf[word2id[term]] = np.log(len(docs) / sum([bool(term in doc) for doc in docs]))\n",
        "    return idf\n",
        "  \n",
        "  word2id = {}\n",
        "  for doc in docs:\n",
        "    for w in doc:\n",
        "      if w not in word2id:\n",
        "        word2id[w] = len(word2id)\n",
        "  \n",
        "  return [[_tf*_idf for _tf, _idf in zip(tf(word2id, doc), idf(word2id, docs))] for doc in docs], word2id\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz5YnQZclGfL"
      },
      "source": [
        "tfidf_vector, word2id = tfidf_vectorizer(preprocessed_docs)\n",
        "print(tfidf_vector)\n",
        "print(word2id.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxCJEOjXYSIP"
      },
      "source": [
        "### Exercise 6\n",
        "BoWã¨TF-IDFã§ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’ãã‚Œãã‚Œè¨ˆç®—ã—ã¦ã¿ã‚ˆã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD4nID08s-21"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAUfDUOmJMsp"
      },
      "source": [
        "### Option 2\n",
        "scikit-learn, nltk gensimãã‚Œãã‚Œã«TF-IDFã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ãŒã‚ã‚‹  \n",
        "ãã‚Œãã‚Œã§TF-IDFã‚’è¨ˆç®—ã—ã¦ã¿ã‚ˆã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2tZLjHNxRDG"
      },
      "source": [
        "# scikit-learnã®tfidfã€€ã‚ã¨ã§æ¶ˆã™\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x7nd2-ekIs-"
      },
      "source": [
        "#nltk ã®tf-idfã€€ã‚ã¨ã§æ¶ˆã™\n",
        "collection = nltk.TextCollection(docs)\n",
        "terms = list(set(collection))\n",
        "nltk_vector = []\n",
        "for doc in docs:\n",
        "  tmp_vec = np.zeros(len(word2id))\n",
        "  for term in word2id.keys():\n",
        "    tmp_vec[word2id[term]] = collection.tf_idf(term, doc)\n",
        "  nltk_vector.append(list(tmp_vec))\n",
        "print(nltk_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEyOY2eQkRMc"
      },
      "source": [
        "#gensim tf-idf ã‚ã¨ã§æ¶ˆã™\n",
        "from gensim import corpora\n",
        "from gensim import models\n",
        "\n",
        "dictionary = corpora.Dictionary(docs)\n",
        "print('===å˜èª->idã®å¤‰æ›è¾æ›¸===')\n",
        "print(dictionary.token2id)\n",
        "print(word2id)\n",
        "\n",
        "corpus = list(map(dictionary.doc2bow, docs))\n",
        "test_model = models.TfidfModel(corpus)\n",
        "corpus_tfidf = test_model[corpus]\n",
        "\n",
        "print('===çµæœè¡¨ç¤º===')\n",
        "gensim_vector = []\n",
        "for doc in corpus_tfidf:\n",
        "  tmp_vec = [0] * len(word2id)\n",
        "  for word in doc:\n",
        "    key = dictionary[word[0]]\n",
        "    tmp_vec[word2id[key]] = word[1]\n",
        "  gensim_vector.append(tmp_vec)\n",
        "\n",
        "print(gensim_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMjo7F6ZKOW1"
      },
      "source": [
        "## Exercise 7\n",
        "\n",
        "æ§˜ã€…ãªå›½ã®Wikipediaã«ãŠã‘ã‚‹abstractã‚’å–ã‚Šå‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨æ„ã—ãŸ  \n",
        "https://drive.google.com/open?id=1i7tekPQRKaAwg-ze3kv5IsufMW13LkLo  \n",
        "ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ä½¿ã†  \n",
        "\n",
        "Cosineé¡ä¼¼åº¦ã®è¨ˆç®—ã‚’è¡Œã„ã€Japanã«ä¼¼ã¦ã„ã‚‹å›½Top5ã‚’è¡¨ç¤ºã—ã¦ã¿ã‚ˆã†  \n",
        "å‰å‡¦ç†ã‚’è‡ªåˆ†ãªã‚Šã«å·¥å¤«ã™ã‚‹ã“ã¨  \n",
        "æ³¨ï¼‰é¡ä¼¼åº¦ã¯ã‚ã¾ã‚Šé«˜ããªã‚‰ãªãã¦ã‚‚è‰¯ã„  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1IbA30KKElT"
      },
      "source": [
        "df = pd.read_csv(\"./nlp_country.csv\")\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2ZqrWJhBAO2"
      },
      "source": [
        "df.iloc[0][\"Abstract\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpQzg57nSAT2"
      },
      "source": [
        "# å¾Œã§æ¶ˆã™\n",
        "def preprocessing_text(text):\n",
        "  def cleaning_text(text):\n",
        "    # @ã®å‰Šé™¤\n",
        "    pattern1 = '@|%'\n",
        "    text = re.sub(pattern1, '', text)    \n",
        "    pattern2 = '\\[[0-9 ]*\\]'\n",
        "    text = re.sub(pattern2, '', text)    \n",
        "    # <b>ã‚¿ã‚°ã®å‰Šé™¤\n",
        "    pattern3 = '\\([a-z ]*\\)'\n",
        "    text = re.sub(pattern3, '', text)    \n",
        "    pattern4 = '[0-9]'\n",
        "    text = re.sub(pattern4, '', text)\n",
        "    return text\n",
        "  \n",
        "  def tokenize_text(text):\n",
        "    text = re.sub('[.,]', '', text)\n",
        "    return text.split()\n",
        "\n",
        "  def lemmatize_word(word):\n",
        "    # make words lower  example: Python =>python\n",
        "    word=word.lower()\n",
        "    \n",
        "    # lemmatize  example: cooked=>cook\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "      return lemma\n",
        "    \n",
        "  text = cleaning_text(text)\n",
        "  tokens = tokenize_text(text)\n",
        "  tokens = [lemmatize_word(word) for word in tokens]\n",
        "  tokens = [remove_stopwords(word, en_stop) for word in tokens]\n",
        "  tokens = [word for word in tokens if word is not None]\n",
        "  return tokens\n",
        "  \n",
        "docs = df[\"Abstract\"].values\n",
        "pp_docs = [preprocessing_text(text) for text in docs]\n",
        "tfidf_vector, word2id = tfidf_vectorizer(pp_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNzPIvaxPOv5"
      },
      "source": [
        "word2id.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p73OT5sPs9y"
      },
      "source": [
        "def calc_cosine(vector, vector_list):\n",
        "  result = {}\n",
        "  for i, x in enumerate(vector_list):\n",
        "    result[i] = cosine_similarity(vector, vector_list[i])\n",
        "    \n",
        "  return result\n",
        "\n",
        "print(\"tfidf\")\n",
        "res = calc_cosine(tfidf_vector[0],tfidf_vector)\n",
        "res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMGEF5UJUZSz"
      },
      "source": [
        "sorted(res.items(), key=lambda x:x[1],reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OWm8pnAMQH-"
      },
      "source": [
        "## Option 3\n",
        "\n",
        "### Word2Vec & Doc2Vec\n",
        "\n",
        "Word2Vecã‚„Doc2Vecã§ã¯å˜èªã®æ„å‘³ã‚’æ‰ãˆã‚‰ã‚Œã¦ã„ã‚‹ã‹ã®ã‚ˆã†ãªæ¼”ç®—ãŒå‡ºæ¥ã‚‹  \n",
        "King - Man + Woman = Queen ãªã©  \n",
        "è©³ç´°ã¯è¬›ç¾©ã‚¹ãƒ©ã‚¤ãƒ‰ã¸   \n",
        "\n",
        "å­¦ç¿’æ¸ˆã¿ã®word2vecãŒgithub( https://github.com/Kyubyong/wordvectors )ã«ä¸ŠãŒã£ã¦ã„ã‚‹ã®ã§  \n",
        "æ—¥æœ¬ã¨å„å›½ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã¦ã¿ã‚ˆã†  \n",
        "è¶³ã—ç®—ã‚„å¼•ãç®—ãŒå‡ºæ¥ã‚‹ã®ã§ãã‚Œã‚‚è©¦ã—ã¦ã¿ã‚ˆã†  \n",
        "\n",
        "å‚è€ƒ : \"BOKU\"ã®ITãªæ—¥å¸¸ (https://arakan-pgm-ai.hatenablog.com/entry/2019/02/08/090000)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWddZ1mZXFsv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}