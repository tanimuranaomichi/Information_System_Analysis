{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained networks and transfer learning (very optional)\n",
    "\n",
    "This notebook is not part of the course, but you can study it if you wish. It is provided as is, without many comments.\n",
    "\n",
    "このノートブックはコースの一部ではありませんが、内容は自由に学ぶことができます。コメントが少なめ、英語のみで提供します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pretrained neural networks\n",
    "\n",
    "In this session, we will learn how to use __pretrained neural networks__. These networks were already trained on a large dataset and the weights of the trained network are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `keras` library contains some pretrained models that are easy to access.\n",
    "<br>\n",
    "These models are in the `keras.applications` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try the `MobileNetV2` network that was pre-trained on the ImageNet databes:\n",
    "- `MobileNetV2` is a specific architecture of CNN for image classification\n",
    "- ImageNet is a large database of images (14 million annotated images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the mobilenet_v2 functions\n",
    "from tensorflow.keras.applications import mobilenet_v2\n",
    "\n",
    "# Create a pre-trained model\n",
    "model_mobilenet = mobilenet_v2.MobileNetV2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at the model layout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mobilenet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that:\n",
    "- There are many layers.\n",
    "- There are `3,504,872` trainable parameters.\n",
    "- The shape of the input placeholder indicates that the images must be of size `224x224` and have `3` channels; Namely `mobilenet_v2` works on color images (RGB channels).\n",
    "- The output shape of the last layer (called `Logits`) indicates that `mobilenet_v2` can recognize `1000` categories of objects. The network was trained using the data of the ImageNet Large Scale Visual Recognition Challenge that uses a subset of ImageNet with `1000` categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try `mobilenet_v2` on a few examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src='./img/ballpen.png' width='200' align=\"left\">|<img src='./img/backpack.png' width='200'>|<img src='./img/cup.png' width='200'>|<img src='./img/keyboard.png' width='200'>|\n",
    "|---|---|---|---|\n",
    "|./img/ballpen.png|./img/backpack.png|./img/cup.png|./img/keyboard.png|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras.preprocessing` package provides tool for loading and formatting images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to load an image, and if necessary resize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = './img/ballpen.png'\n",
    "img = image.load_img(img_path, target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `image` tool loads images as a `PIL.Image.Image` object.\n",
    "<br>\n",
    "We need to transform it to a `numpy` array for the network.\n",
    "<br>\n",
    "The `PIL.Image.Image` object has a function `img_to_array` that does this transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(img))\n",
    "x = image.img_to_array(img)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `numpy` array `x` has a shape of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mobilenet_v2` network expects an array of shape `(None, 224, 224, 3)`.\n",
    "<br>\n",
    "This means that it is a block of at least one image of size `224x224` with `3` channels.\n",
    "<br>\n",
    "Let us create a block of one image from our image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.expand_dims(x, axis=0) \n",
    "# There other ways to do this conversion:\n",
    "#X = x[np.newaxis, :, :, :]\n",
    "#X = x.reshape((1,224,224,3))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras` provides a function to preprocess input images for `mobilenet_v2`.\n",
    "<br>\n",
    "The preprocessing substract a mean value (computed during training) from the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mobilenet_v2.preprocess_input(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the image is preprocessed, we can use the `mobilenet_v2` network to get the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model_mobilenet.predict(X)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the prediction is a vector of size `(1, 1000)` as we used an input of size `(None, 224, 224, 3)`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras` also provides a function to display the predictions.\n",
    "The parameter `top` limits the number of predictions to consider; here, we only access the top three:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_predictions = mobilenet_v2.decode_predictions(predictions, top=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of `decode_predictions` is a list containing a list of elements for each of the input images.\n",
    "<br>\n",
    "Each element has 3 fields:\n",
    "- An id from the ImageNet database for the predicted object\n",
    "- A human readable name for the object\n",
    "- The value of the output neuron corresponding to that object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we gave only one input image so `decoded_predictions[0]` contains the list of element for that image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in decoded_predictions[0]:\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it\n",
    "\n",
    "Check what are the predictions of mobilenet_v2 for the other 3 example images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it\n",
    "\n",
    "Process the 4 example images together in a single call to `model.predict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package `keras.applications` contains many different models.\n",
    "<br>\n",
    "For example:\n",
    "- the `VGG19` model from the package `vgg19`\n",
    "- the `ResNet50` model from the package `resnet50`\n",
    "\n",
    "have the same interface as `mobilenet_v2`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will learn to re-use a part of a trained network for solving another classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying pretrained models on our data\n",
    "\n",
    "Most of the time, the pre-trained deep networks like \"VGG19\", \"ResNet50\" and \"MobileNetV2\" are trained on a dataset (here ImageNet subset) that does not correspond to the classification task we want to do on our own data.\n",
    "<br>\n",
    "For example, if we want to create an image classifier that classify images of hard discs and ram modules, the pre-trained classifier may not be the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `./data/` folder, there are two subfolders `hd/` and `ram/` each containing various images of hard discs and ram modules.\n",
    "<br>\n",
    "Let us see what kind of results `mobilenet_v2` gives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the files easily, we use the `glob` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "image_list = glob.glob('./data/hd/hd_*.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can apply `mobilenet_v2` on all images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "for img_path in image_list:\n",
    "    try:\n",
    "        X = image.img_to_array(image.load_img(img_path, target_size=(224, 224))).reshape((1,224,224,3))\n",
    "        X = mobilenet_v2.preprocess_input(X)\n",
    "        predictions = model_mobilenet.predict(X)\n",
    "        decoded_predictions = mobilenet_v2.decode_predictions(predictions, top=1)\n",
    "        pred_list.append(decoded_predictions[0][0][1])#Just keep the readable name of the class\n",
    "    except OSError as e:\n",
    "        print(str(e))\n",
    "\n",
    "# Show one example\n",
    "plt.imshow(image.load_img(img_path, target_size=(224, 224)))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Keep one example for later use\n",
    "X_hd = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_set = set(pred_list)\n",
    "labels = []\n",
    "counts = []\n",
    "for obj in objects_set:\n",
    "    labels += [obj]\n",
    "    counts += [pred_list.count(obj)]\n",
    "\n",
    "plt.barh(2*np.arange(len(counts)),counts,1.5)\n",
    "plt.yticks(2*np.arange(len(labels)), labels, rotation='horizontal')\n",
    "plt.ylim(-1, 2*(len(counts)-0.5))\n",
    "plt.xlabel(\"counts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mobilenet_v2` is able to recognize the hard disc images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mobilenet_v2 accuracy:', pred_list.count('hard_disc') / len(pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "image_list = glob.glob('./data/ram/ram_*.jpg')\n",
    "\n",
    "pred_list = []\n",
    "for img_path in image_list:\n",
    "    try:\n",
    "        X = image.img_to_array(image.load_img(img_path, target_size=(224, 224))).reshape((1,224,224,3))\n",
    "        X = mobilenet_v2.preprocess_input(X)\n",
    "        predictions = model_mobilenet.predict(X)\n",
    "        decoded_predictions = mobilenet_v2.decode_predictions(predictions, top=1)\n",
    "        pred_list.append(decoded_predictions[0][0][1])\n",
    "    except OSError as e:\n",
    "        print(str(e))\n",
    "\n",
    "# Show one example\n",
    "plt.imshow(image.load_img(img_path, target_size=(224, 224)))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Keep one example for later use\n",
    "X_ram = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_set = set(pred_list)\n",
    "labels = []\n",
    "counts = []\n",
    "for obj in objects_set:\n",
    "    labels += [obj]\n",
    "    counts += [pred_list.count(obj)]\n",
    "\n",
    "plt.barh(2*np.arange(len(counts)),counts,1.5)\n",
    "plt.yticks(2*np.arange(len(labels)), labels, rotation='horizontal')\n",
    "plt.ylim(-1, 2*(len(counts)-0.5))\n",
    "plt.xlabel(\"counts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mobilenet_v2` does not know about RAM! (The reason is that RAM images are not in the ImageNet dataset.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncated pretrained network\n",
    "\n",
    "The models in the `keras.applications` package have a `include_top` parameter.\n",
    "If set to `True`, the model includes the classification part otherwise only the feature part is loaded.\n",
    "<br>\n",
    "Let us load a `mobilenet_v2` without the classification part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mobilenet_no_top = mobilenet_v2.MobileNetV2(include_top=False)\n",
    "model_mobilenet_no_top.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the full `mobilenet_v2`, last few layers are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the block size that was already `None` in the full `mobilenet_v2`, the image width and height are also set to `None` in the truncated `mobilenet_v2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applied to an image of size `224x224` with `3` channles, the `predict` function outputs an image of size `7x7` with `1056` channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_hd = model_mobilenet_no_top.predict(X_hd)\n",
    "print(features_hd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize this image, let us concatenate all the `1056` channels in a large image.\n",
    "<br>\n",
    "We create a large image of size `(7x32)x(7x64)` by having `32` rows and `33` columns of small `7x7` images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "I_hd = np.zeros((7*32, 7 *33))\n",
    "for i in range(32):\n",
    "    for j in range(33):\n",
    "        I_hd[i*7 : (i+1)*7, j*7 : (j+1)*7] = features_hd[0,:,:,i*33+j].reshape((7,7))\n",
    "plt.imshow(I_hd)\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image shows the representation of the input hard disc image obtained from the convolutive part of the trained `mobilenet_v2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing for the ram image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ram = model_mobilenet_no_top.predict(X_ram)\n",
    "\n",
    "I_ram = np.zeros((7*32, 7 *33))\n",
    "for i in range(32):\n",
    "    for j in range(33):\n",
    "        I_ram[i*7 : (i+1)*7, j*7 : (j+1)*7] = features_ram[0,:,:,i*33+j].reshape((7,7))\n",
    "plt.imshow(I_ram)\n",
    "plt.axis(\"off\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second image is the representation of the input ram image obtained from the convolutive part of the trained `mobilenet_v2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of transfer learning is to train a classifier not to on the original images but to on the representations (__features__) obtained from the truncated network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an intermediary feature dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fist approach is to first transform our image dataset into a feature dataset by applying the truncated network to all the images.\n",
    "\n",
    "We preprocess all the hard disc and RAM images using the truncated MobileNetV2. We obtain a dataset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = glob.glob('./data/hd/hd_*.jpg')\n",
    "w = 224\n",
    "img_list_hd = []\n",
    "X_list_hd = []\n",
    "for img_path in image_list:\n",
    "    try:\n",
    "        img = image.load_img(img_path, target_size=(w, w))\n",
    "\n",
    "        X = image.img_to_array(img).reshape((1,w,w,3))\n",
    "        img_list_hd.append(X.copy()/255.0)\n",
    "\n",
    "        X = mobilenet_v2.preprocess_input(X)\n",
    "\n",
    "        F = model_mobilenet_no_top.predict(X)\n",
    "        X_list_hd.append(F)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "imgs_hd = np.concatenate(img_list_hd, axis=0)\n",
    "X_hd = np.concatenate(X_list_hd, axis=0)\n",
    "y_hd = np.zeros(X_hd.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = glob.glob('./data/ram/ram_*.jpg')\n",
    "\n",
    "img_list_ram = []\n",
    "X_list_ram = []\n",
    "for img_path in image_list:\n",
    "    try:\n",
    "        img = image.load_img(img_path, target_size=(w, w))\n",
    "\n",
    "        X = image.img_to_array(img).reshape((1,w,w,3))\n",
    "        img_list_ram.append(X.copy()/255.0)\n",
    "\n",
    "        X = mobilenet_v2.preprocess_input(X)\n",
    "\n",
    "        F = model_mobilenet_no_top.predict(X)\n",
    "        X_list_ram.append(F)\n",
    "    except OSError:\n",
    "        pass\n",
    "imgs_ram = np.concatenate(img_list_ram, axis=0)\n",
    "X_ram = np.concatenate(X_list_ram, axis=0)\n",
    "y_ram = np.ones(X_ram.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.concatenate((imgs_hd, imgs_ram), axis=0)\n",
    "X = np.concatenate((X_hd, X_ram), axis=0)\n",
    "y = np.concatenate((y_hd, y_ram), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the features dataset into training and testing part.\n",
    "<br>\n",
    "(We do it manually so that we can keep track of the corresponding original images.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a permutation of the indicies\n",
    "number_samples = X.shape[0]\n",
    "shuffle_index = np.random.permutation(number_samples)\n",
    "\n",
    "# use the permuted list as indices\n",
    "imgs = imgs[shuffle_index, :,:,:]\n",
    "X = X[shuffle_index,:,:,:]\n",
    "y = y[shuffle_index]\n",
    "\n",
    "# Split the data in training and testing\n",
    "testing_training_ratio = 0.5\n",
    "test_samples = int(testing_training_ratio * number_samples)\n",
    "\n",
    "# from 0 to test_samples-1\n",
    "imgs_test = imgs[:test_samples]\n",
    "X_test = X[:test_samples]\n",
    "y_test = y[:test_samples]\n",
    "\n",
    "# From test_samples to end\n",
    "imgs_train = imgs[test_samples:]\n",
    "X_train = X[test_samples:]\n",
    "y_train = y[test_samples:]\n",
    "\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Testing set size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a network on the features\n",
    "\n",
    "We now train a small neural network to recognize hard discs and ram based on the features.\n",
    "\n",
    "We will use a 3 layer fully connected neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "input_shape = X[0].shape\n",
    "feature_input = Input(shape=input_shape, name=\"feature_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = Flatten()(feature_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = Dense(64, activation='relu', name='fc1')(fl)\n",
    "dp1 = Dropout(0.1)(fc1)\n",
    "fc2 = Dense(64, activation='relu', name='fc2')(dp1)\n",
    "dp2 = Dropout(0.1)(fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc3 = Dense(2, activation='softmax', name='fc3')(dp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature = Model(feature_input, fc3, name='hd_or_ram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train.\n",
    "<br>\n",
    "(Note: we use the _Adagrad_ optimizer, as it worked better then Adam for this problem.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = to_categorical(y_train)\n",
    "\n",
    "model_feature.compile(loss='binary_crossentropy', optimizer=Adagrad(lr=0.01), metrics=['acc'])\n",
    "\n",
    "model_checkpoint_cb = ModelCheckpoint(\"model_feature_weights.hdf5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=10, verbose=0, mode='auto')\n",
    "H = model_feature.fit(X_train, y_train_one_hot, batch_size=16, epochs=50, validation_split=0.25 , shuffle=True, callbacks=[model_checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(H.history['loss'], label=\"loss\")\n",
    "plt.plot(H.history['val_loss'], label=\"val_loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"loss vs epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(H.history['acc'], label=\"acc\")\n",
    "plt.plot(H.history['val_acc'], label=\"val_acc\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.title(\"Accuracy vs epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance\n",
    "\n",
    "Let's reload the best model and test the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature.load_weights(\"model_feature_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_one_hot = model_feature.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(y_test_pred_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "CM = confusion_matrix(y_test, y_test_pred)\n",
    "print(CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.sum(np.diag(CM)) / np.sum(CM)\n",
    "print(\"Accuracy = {:.02f}\".format(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.diag(CM) / np.sum(CM, axis = 0)\n",
    "R = np.diag(CM) / np.sum(CM, axis = 1)\n",
    "for i in range(2):\n",
    "    print(\"Class '{}' : P = {:.02f} R = {:.02f}\".format(i, P[i], R[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plot a few examples of correct classifications. and all the incorrect classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_indices = np.where(y_test_pred == y_test)[0]\n",
    "for i in correct_indices[:10]:\n",
    "    plt.figure()\n",
    "    plt.imshow(imgs_test[i, :, :, :])\n",
    "    plt.axis('off')\n",
    "    if y_test[i] == 0:\n",
    "        title = \"True HD Pred HD\"\n",
    "    else:\n",
    "        title = \"True RAM Pred RAM\"\n",
    "    plt.title(title)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_indices = np.where(y_test_pred != y_test)[0]\n",
    "for i in error_indices:\n",
    "    plt.figure()\n",
    "    plt.imshow(imgs_test[i, :, :, :])\n",
    "    plt.axis('off')\n",
    "    if y_test[i] == 0:\n",
    "        title = \"True HD Pred RAM\"\n",
    "    else:\n",
    "        title = \"True RAM Pred HD\"\n",
    "    plt.title(title)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it\n",
    "Use transfer learning to create a classifier for two (or more) classes that are not in the `1000` default classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
