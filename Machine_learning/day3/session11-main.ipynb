{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural networks / 畳み込みニューラルネットワーク (optional)\n",
    "\n",
    "In this session, we will discuss _convolutional neural networks_ (CNNs).\n",
    "\n",
    "CNNs are a good choice for classification and other tasks when the input is an image. For such cases is often possible to obtain a better result with CNNs then with fully connected networks. CNNs can also be used for many types of data that can be transformed into an image (e.g. sound data, or sequences of observations, etc.).\n",
    "\n",
    "このセッションでは、_畳み込みニューラルネットワーク_ （convolutional neural network, CNN）について説明します。\n",
    "\n",
    "CNNは、入力が画像の時に行う分類などタスクに適しています。入力が画像の場合、全結合ネットワークよりもCNNを使用する方がよい場合がよくあります。また、画像に変換できる多くの種類のデータ（たとえば、音声データ、または一連の観測など）にも使用できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layers / 畳み込み層\n",
    "\n",
    "CNNs use __convolutional layers__, which operate differently from fully connected layers. \n",
    "<br>\n",
    "The input to the convolutional layer is an image - an array of pixels, where each pixel is represented by one or more values (called  __channels__): e.g. in a standard color image we have the _red_ , _blue_ and _green_ channel. \n",
    "<br>\n",
    "Unlike in fully connected layers, each neuron in the convolution layer is connected only to one part of the input image - a rectangular window of pixels, see figure below. As a result, the output of the convolutional layer is also an image-like array.\n",
    "\n",
    "CNNは、完全に接続された層とは異なる動作をする__畳み込み層__を使用します。\n",
    "<br>\n",
    "たたみ込み層への入力は画像のピクセル配列であり、各ピクセルは1つ以上の値（__チャンネル__と呼ばれる）で表されます。例えば、標準のカラー画像には、_赤_、_青_、_緑_ のチャンネルがあります。\n",
    "<br>\n",
    "全結合層とは異なり、畳み込み層の各ニューロンは、入力画像の一部にのみ接続されます。接続されているのはピクセルの長方形のウィンドウ、下の図は例を表しています。その結果、畳み込み層の出力も画像のような配列です 。\n",
    "<br>\n",
    "\n",
    "<img src=\"./img/convolution.jpg\" width=1000>\n",
    "\n",
    "Typically several convolutions are applied to the same window, resulting in multiple channels of the output image.\n",
    "<br>\n",
    "For example, the next figure shows an example of a convolutional layer which takes as input an image with one channel and outputs an image with 4 channels.\n",
    "\n",
    "通常、同じウィンドウに複数の畳み込みが適用されるため、出力画像は複数のチャンネルがあります。\n",
    "<br>\n",
    "たとえば、次の図は、1つのチャンネルを持つ画像を入力として受け取り、4つのチャンネルを持つ画像を出力する畳み込み層の例を示しています。\n",
    "\n",
    "<img src=\"./img/convolution_multi.jpg\" width=200>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutions / 畳み込み\n",
    "\n",
    "The neurons perform the __convolution__ operation, which is the form $f(\\sum c W + b)$, where $W$ contains the pixel values in the input window, $c$ and $b$ are trainable coefficients and $f$ is an activation function.\n",
    "<br>\n",
    "Importantly, in each channel the parameters $c$ and $b$ are _shared for all windows_ ! \n",
    "<br>\n",
    "For example, for a window of size `3x3`, each channel has `3x3+1 = 10` trainable parameters. The number of parameters does not depend on the size of the image, and it is _much smaller_ than what a fully connected network would have.\n",
    "\n",
    "In the example in the previous figure, there is one set of trainable values for each of the channels. If we assume that all the windows have a size of `3x3` then there is `4x(3x3+1) = 40` trainable parameters.\n",
    "\n",
    "畳み込み層のニューロンは__畳み込み__演算を実行します。その形式は$f(\\sum c W + b)$です。$W$には入力ウィンドウのピクセル値が含まれてます。$c$と$b$はトレーニング可能な係数です。$f$は活性化関数です。\n",
    "<br>\n",
    "重要なのは、各チャンネルでパラメータ$c$と$b$が_すべてのウィンドウで共有される_ことです。\n",
    "<br>\n",
    "たとえば、ウィンドウの大きさが`3x3`の場合、各チャネルには`3x3+1 = 10`のトレーニング可能なパラメーターがあります。パラメータの数は画像のサイズに依存せず、全結合ネットワークよりもはるかにパラメータが少ないです。\n",
    "\n",
    "前の図の例では、チャンネルごとに1セットのトレーニング可能な係数があります。すべてのウィンドウの大きさが`3x3`であると仮定すると、トレーニング可能なパラメーターが`4x(3x3+1) = 40`あります。\n",
    "\n",
    "The input may also have several channels (e.g. an RGB image).\n",
    "In this case, the input for the convolution operation is calculated on all channels of the input window, see the next figure.\n",
    "<br>\n",
    "In this example, assuming all slidding windows have a size of `3x3`, there are `1x(3x3x4)+1 = 37` trainable parameters.\n",
    "\n",
    "入力には複数のチャンネルも含まれる場合があります（例えばRGB画像には3つのチャンネルがあります）。\n",
    "この場合、畳み込み演算の入力は、入力ウィンドウのすべてのチャネルで計算されます。次の図を参照してください。\n",
    "<br>\n",
    "この例では、すべてのスライドウィンドウの大きさが`3x3`であるとすると、トレーニング可能なパラメーターが`1x(3x3x4)+1 = 37`あります。\n",
    "\n",
    "<img src=\"./img/convolution_multi_input.jpg\" width=200>\n",
    "\n",
    "Note that, since the computations are limited to a window, the convolutions are exploiting local information present in the image.\n",
    "\n",
    "計算はウィンドウに限定されているため、畳み込みは画像に存在するローカル情報を利用していることに注意してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling layers / プーリング層\n",
    "\n",
    "In CNNs convolution layers are often combined with layers doing pooling (typically _max pooling_) operations.\n",
    "<br>\n",
    "A pooling operation also operates on windows, as illustrated in the image below,\n",
    "\n",
    "Max pooling just outputs the largest (max) value among the pixels in the window (for each channel separately). Thus, the max pooling layer has no trainable parameters.\n",
    "\n",
    "CNNでは畳み込み層以外、多くの場合プーリング計算を実行する層を使用しています。（通常は _最大プーリング_ を使います。）\n",
    "<br>\n",
    "次の図に示すように、プーリング計算もウィンドウ上に実行します。\n",
    "\n",
    "最大プーリングでは、ウィンドウ内のピクセルの最大値が出力されます。（最大値の計算はチャネルごとに行われます。）したがって、最大プーリング層にはトレーニング可能なパラメーターがありません。\n",
    "\n",
    "<img src=\"./img/max_pool.jpg\" width=200>\n",
    "\n",
    "\n",
    "Typically, pooling is not applied to all possible windows, but only to nonoverlapping windows. This results in an output image of a smaller size. In fact, the main purpose of using pooling layers is to diminish the size of the image that will be passed on to the following layers.\n",
    "<br>\n",
    "For example, doing pooling using `2x2` windows will result in an image with `4` times less pixels.\n",
    "\n",
    "通常、プーリングはすべての可能なウィンドウに適用されるのではなく、重ならないウィンドウにのみ適用されます。これにより、出力イメージのサイズが小さくなります。実際、プーリング層を使用する主な目的は画像のサイズを小さくすることです。\n",
    "<br>\n",
    "たとえば、`2x2`ウィンドウを使用してプーリングを行うと、出力画像は入力画像よりピクセルが`4`倍少ないです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image dataset / 画像のデータセット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again work with the MNIST dataset. \n",
    "<br>\n",
    "However, now we will use the data as 2D images, not as vectors, like in the earlier examples.\n",
    "\n",
    "再びMNISTデータセットを使用します。\n",
    "<br>\n",
    "ただし、ここでは、前の例のように、ベクターではなく2次元の画像としてデータを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mnist_loader import MNISTImageLoader\n",
    "mnist_image_loader = MNISTImageLoader(43)\n",
    "X, y = mnist_image_loader.samples(70000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one sample is composed of an image of size `28x28` and a label. The image is grayscale, so it has only one channel.\n",
    "\n",
    "これで、1つのサンプルが大きさ`28x28`の画像とラベルで構成されます。画像はグレースケールであるため、チャンネルは1つだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 dimensions of `X` are:\n",
    "- the width of the sample image: `28`\n",
    "- the height of the sample image: `28`\n",
    "- the number of channels in the sample image: `1` \n",
    "\n",
    "`X`の3つの次元は次のとおりです。\n",
    "- 画像の幅： `28`\n",
    "- 画像の高さ： `28`\n",
    "- 画像のチャネル数： `1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us split the dataset between training and testing sets.\n",
    "\n",
    "データセットをトレーニングセットとテストセットに分割しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (35000, 28, 28, 1)\n",
      "Testing set size: (35000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# The pixel values are in the range [0, 255], we will limit them to [0, 1]\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Testing set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network / ネットワークの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of creating the convolutional neural network is very similar to the fully connected network.\n",
    "\n",
    "畳み込みニューラルネットワークを作成する手順は、全結合ネットワークと似ています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input layer for the CNN will contain an image.\n",
    "\n",
    "CNNの入力層には画像が入ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train[0].shape\n",
    "img_input = Input(shape = input_shape, name='input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create a first convolution layer.\n",
    "<br>\n",
    "In `keras`,  the object `Conv2D` represents a 2D convolution that expects an image as input (here `img_input`):\n",
    "- The first parameter `32` is the number of channels of the output image\n",
    "- The `kernel_size` defines the size of the window\n",
    "- The `activation` defines the type of function to apply on the output for each window (as in fully connected layers, `relu` is often used)\n",
    "\n",
    "\n",
    "最初の畳み込み層を作成しましょう。\n",
    "<br>\n",
    "`keras`では、オブジェクト`Conv2D`は2次元の画像を入力として扱う畳み込み層です。\n",
    "- 最初のパラメータ`32`は出力画像のチャンネル数です\n",
    "- `kernel_size`はウィンドウのサイズを定義します\n",
    "- `activation`は、各ウィンドウの出力に適用する関数のタイプを定義します（全結合層と同じく、`relu`がよく使われる）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu', name='conv1')(img_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer `conv1` appplies `32` different filters of size `3x3` to the input image that has one channel.\n",
    "Consequently the layer has `32x(3x3)+32=320` trainable coeffecients (as a comparison, the fully connected network was using `784x128=100480` coefficients in its first layer). \n",
    "\n",
    "`conv1`層は、1つのチャンネルを持つ入力画像に大きさ`3x3`の畳み込みを32個適用します。\n",
    "その結果、この層には`32x(3x3)+32=320`のトレーニング可能な係数があります。（比較として、前のセッションの全結合ネットワークは最初の層で`784x128=100480`の係数を使用していました。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add a second convolution layer.\n",
    "\n",
    "2番目の畳み込み層を追加しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = Conv2D(64, (3, 3), activation='relu', name='conv2')(conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer takes as input the output of the previous convolution that has `32` channels and outputs `64` channels.\n",
    "It also uses windows of size `3x3`.\n",
    "You can easily check that it has `18496` trainable parameters.\n",
    "\n",
    "この層は、`32`チャンネルを持つ前の畳み込みの出力を入力として受け取り、`64`チャンネルを出力します。\n",
    "サイズが`3x3`のウィンドウを使用します。\n",
    "トレーニング可能なパラメータが`18496`あることを簡単に確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add a max pooling layer. \n",
    "<br>\n",
    "The `pool_size` defines the size of the window for extracting the maxima. \n",
    "<br>\n",
    "By default, there is no overlap of the windows. \n",
    "Consequently, this layer will divide the width and height of the image by `2`.\n",
    "\n",
    "\n",
    "最大プーリング層を追加しましょう。\n",
    "<br>\n",
    "`pool_size`は最大値を計算するためのウィンドウのサイズを定義します。\n",
    "<br>\n",
    "デフォルトでは、ウィンドウの重なりはありません。\n",
    "その結果、この層は画像の幅と高さを半分にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = MaxPooling2D(pool_size=(2, 2), name='pool')(conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout\n",
    "\n",
    "Dropout is a technique used to avoid _overfitting_ .\n",
    "<br>\n",
    "The dropout layer randomly \"drops out\" some of the connections by setting them to `0`. This is done only during training, and it has the effect of preventing the network to overfit to the training data. \n",
    "(When using the network to make predictions, the dropping out is not used.)\n",
    "\n",
    "Dropout is typically a very successful way for preventing overfitting. It is used in both fully connected and convolutional networks.\n",
    "\n",
    "Dropout(ドロップアウト)は、_過学習_を回避するために使用される手法です。\n",
    "<br>\n",
    "Dropout層は、ランダムにいくつかの接続を「0」に設定することにより、それらを「ドロップアウト」します。これはトレーニング中にのみ行われ、ネットワークがトレーニングデータに適合しすぎるのを防ぐ効果があります。\n",
    "（予測を行う場合、ドロップアウトは使用されません。）\n",
    "\n",
    "Dropoutは通常、過学習を防ぐための非常に成功した方法です。全結合ネットワークにも畳み込みネットワークにも使用できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the `Dropout` layer to our network.\n",
    "<br>\n",
    "The only parameter that we need to specify is the _dropout rate_ , which determines the probability that the outputs will be dropped during training (here it is set to `0.25`, i.e. 25%).\n",
    "\n",
    "`Dropout`層をネットワークに追加しましょう。\n",
    "<br>\n",
    "指定する必要がある唯一のパラメーターは _ドロップアウト率_ です。これは、トレーニング中に出力がドロップされる確率を決定します（ここでは、`0.25`、つまり25％に設定されています）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp1 = Dropout(0.25)(pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final layers / 最終層\n",
    "\n",
    "In a CNN the final layers are typically fully connected layers. \n",
    "\n",
    "Since we use vectors as inputs to the fully connected layers, we should transform the image output from the previous layer to a vector.\n",
    "<br>\n",
    "This is the role of the `Flatten` layer.\n",
    "(It takes as input an image with possibly several channels and outputs that image as a vector by stacking all the values.)\n",
    "\n",
    "\n",
    "CNNでは、最終層は通常、全結合層を使います。\n",
    "\n",
    "全結合層への入力としてベクターを使用するため、前のレイヤーからの画像出力をベクターに変換する必要があります。\n",
    "<br>\n",
    "これが`Flatten`層の役割です。\n",
    "（複数のチャネルを持つ画像を入力として受け取り、すべての値を積み重ねることにより、その画像をベクトルとして出力します。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = Flatten()(dp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a fully connected layer with `128` neurons.\n",
    "\n",
    "`128`ニューロンを含む全結合層を追加します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = Dense(128, activation='relu', name=\"fc1\")(fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add another `Dropout` layer to avoid overfitting and help generalization.\n",
    "Note that the `Dropout` layer works well on both image data or vector data.\n",
    "\n",
    "過学習を回避するために、もう一つのDropout層を追加しましょう。\n",
    "Dropout層は画像データとベクターデータの両方で使用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp2 = Dropout(0.5)(fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as in the previous session, we add the decision layer which is a fully connected layer with `10` neurons and a `softmax` activation function.\n",
    "\n",
    "最後に、前のセッションと同様に、`10`ニューロンと`softmax`活性化関数を備えた全結合層を追加します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc2 = Dense(10, activation='softmax', name=\"fc2\")(dp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we just have to create the model and specify is the input and output.\n",
    "\n",
    "最後に、入力と出力を指定してモデルを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Model(img_input, fc2, name='CNN_classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us display the structure of the created network.\n",
    "\n",
    "作成したネットワークの構造を表示してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "pool (MaxPooling2D)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several observations we can make about this network:\n",
    "- Each of the convolution layers reduces the size of the images of `2` pixels: From `28` to `26` to `24`.\n",
    "- The max pooling layer divides the dimensions by `2` from `24` to `12`. It does not have trainable parameters.\n",
    "- Dropout also does not have trainable parameters.\n",
    "- The vector output of the flatten layer has a dimension of `12x12x64 = 9216`.\n",
    "- The fully connected layer `fc1` has the most trainable parameters `1179776`.\n",
    "\n",
    "このネットワークについては、いくつかの観察があります。\n",
    "- 各畳み込み層は、画像のサイズを`2`ピクセルで縮小します（`28`から`26`から`24`）。\n",
    "- 最大プーリング層は、画像の幅・高さを`2`で割ります（`24`から`12`）。 トレーニング可能なパラメータはありません。\n",
    "- ドロップアウトにもトレーニング可能なパラメーターはありません。\n",
    "- フラット層のベクター出力のディメンションは`12x12x64 = 9216`です。\n",
    "- 全結合層`fc1`には、最もトレーニング可能なパラメーターがあります（`1179776`）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try  it yourself ! / 自分で試そう！\n",
    "\n",
    "First, _stop or restart the kernel_ in this notebook.\n",
    "\n",
    "Then, [click here](session11-playground.ipynb) to open a sample notebook where you can train a convolutional neural network on MNIST.\n",
    "\n",
    "まずはこのノートブックの _カーネルを停止・再起動_ してください。\n",
    "\n",
    "その後、[ここをクリックして](session11-playground.ipynb)サンプルのノートブックを開き、MNISTデータセット上にCNNをトレーニングしてみてください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
