{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with neural networks / ニューラルネットワークによる分類 (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First run the code from the previous session.\n",
    "<br>\n",
    "まず、前のセッションのコードを実行します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the needed packages.\n",
    "<br>\n",
    "必要なパッケージをインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST dataset.\n",
    "<br>\n",
    "MNISTデータセットをロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mnist_loader import MNISTVectorLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mnist_vector_loader = MNISTVectorLoader(43)\n",
    "# X, y = mnist_vector_loader.samples(70000)\n",
    "X, y = mnist_vector_loader.samples(70)\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the network.\n",
    "<br>\n",
    "ネットワークを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train[0].shape\n",
    "vector_input = Input(shape = input_shape, name='input')\n",
    "\n",
    "fc1 = Dense(128, activation='relu', name='fc1')(vector_input)\n",
    "fc2 = Dense(128, activation='relu', name='fc2')(fc1)\n",
    "output = Dense(10, activation='softmax', name='output')(fc2)\n",
    "\n",
    "network = Model(vector_input, output, name='classification')\n",
    "\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network / ネットワークの訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST labels are digits:\n",
    "\n",
    "MNISTのラベルは数字です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 8 3 2 0 5 7 3 4 4]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using our dataset, we have to transform the label so that `keras` can compare them to the output of `output`.\n",
    "The cost function has to compare the label to the outputs of the softmax function in `output`, which is a vector of size `10`.\n",
    "<br>\n",
    "Consequently we need to perform a __\"one-hot\" encoding__ of the labels in order to transform a label in a vector of size `10`.\n",
    "\n",
    "The one-hot encoding is a vector having a length equal to the number of classes; here `10`.\n",
    "<br>\n",
    "The elements of the vector are set to `0` except for the position corresponding to the encoded class that is set to `1`.\n",
    "<br>\n",
    "For example for the digit is `5` the one-hot encoding is `[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]`\n",
    "\n",
    "During training, the one-hot encoding of the digit is the compared to the output of the network.\n",
    "\n",
    "\n",
    "データセットを使う前に、 `keras`がそれらを` output`の出力と比較できるようにラベルを変換する必要があります。\n",
    "コスト関数はサイズが10のベクトルである`output`の中のsoftmax関数の出力とラベルを比較しなければなりません。\n",
    "<br>\n",
    "そのため、ラベルを10の大きさのベクトルに変換する必要があります。ラベルをベクトルに変更するには__one-hot(ワンホット)エンコーディング__を使用します。\n",
    "\n",
    "ワンホットエンコーディングは、クラス数に等しい長さを持つベクトルです。\n",
    "<br>\n",
    "ベクトルの要素は、対象クラスに対応する要素のみが`1`に設定され、残りは`0`に設定される。\n",
    "<br>\n",
    "例えば、数字が`5`の場合、ワンホットエンコーディングは`[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]`です。\n",
    "\n",
    "トレーニング中、数字のワンホットエンコーディングはネットワークの出力と比較されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `to_categorical` from `keras.utils` does this encoding:\n",
    "\n",
    "ワンホットエンコーディングを行うためには`keras.utils`の`to_categorical`関数が使えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train_one_hot = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at a few labels:\n",
    "\n",
    "得られたラベルをいくつか見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original digit label: 9 Created one-hot vector label: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Original digit label: 8 Created one-hot vector label: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "Original digit label: 3 Created one-hot vector label: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "Original digit label: 2 Created one-hot vector label: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Original digit label: 0 Created one-hot vector label: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Original digit label: 5 Created one-hot vector label: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Original digit label: 7 Created one-hot vector label: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Original digit label: 3 Created one-hot vector label: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "Original digit label: 4 Created one-hot vector label: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Original digit label: 4 Created one-hot vector label: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"Original digit label:\", y_train[i], \"Created one-hot vector label:\", y_train_one_hot[i, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the labels are in the right format, let us prepare the training.\n",
    "\n",
    "ラベルが正しい形式になったので、トレーニングの準備に移ります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the algorithm for training / トレーニング用アルゴリズムの定義\n",
    "\n",
    "The `keras` library contains many algorithms for training neural networks.\n",
    "<br>\n",
    "These algorithms are __optimization algorithms__ that _adjust the trainable parameters of a model in order to minimize a loss function_.\n",
    "\n",
    "In the case of classification, while training the network with known examples from the training set, the __cost function__ measures how \"close\" is the predicted output of the network to the true value.\n",
    "<br>\n",
    "Then, the optimizer updates the trainable weights in order to make the predicted output a \"bit closer\" to the true value.\n",
    "\n",
    "`keras`ライブラリはニューラルネットワークを訓練するための多くのアルゴリズムを持っています。\n",
    "<br>\n",
    "これらのアルゴリズムは __最適化アルゴリズム__ です。モデルの _トレーニング可能なパラメータを調整し損失関数を最小化_ します。\n",
    "\n",
    "分類の場合、トレーニングセットのサンプルを用いてネットワークを訓練している間、__コスト関数__ はネットワークの出力が真の値にどれだけ「近い」かを測定する。\n",
    "<br>\n",
    "最適化アルゴリズムは予測出力を真の値に「少し近づける」ようにネットワークの重みを更新します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `keras`, to use an optimizer, it is necessary to:\n",
    "- define the loss function, also called `loss`\n",
    "- choose the optimizer\n",
    "- select performance metrics\n",
    "\n",
    "This is done by calling the `compile` method of the `Model` object:\n",
    "\n",
    "`keras`では、オプティマイザを使うためには、次のことが必要です。\n",
    " - 損失関数（`loss`）を定義する\n",
    " - オプティマイザを選択\n",
    " - パフォーマンス指標を選択\n",
    " \n",
    "これは`Model`オブジェクトの` compile`メソッドを呼び出すことによって行われます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters we used here are very typical. In most cases this will work well for classification using typical neural networks.\n",
    "\n",
    "- The __categorical crossentropy__ loss function (`categorical_crossentropy`) is the basic choice for classification with multiple classes.\n",
    "- The optimizer is of type `Adam`, and the parameter `lr` controls how \"fast\" the weights are modified. (`keras` provides several different optimizers but we will not explain them here in detail. `Adam` usually performs well.)\n",
    "- The perfromance petrics that we are trying to optimize is accuracy (`acc`).\n",
    "\n",
    "ここに使用したパラメータはかなり典型的なものです。ほとんどの場合、一般的なニューラルネットワークを使用した分類にこのパラメータが適しています。\n",
    "\n",
    "- 複数のクラスでの分類には一般 __多クラス交差エントロピー__ `categorical_crossentropy`損失関数を使います。\n",
    "- オプティマイザは`Adam`型で、パラメータ`lr`は重みがどのくらい速く変更されるかを制御します。（`keras`は複数のオプティマイザを提供します。ここではオプティマイザについて詳細に説明しません。`Adam`は通常、うまく機能します。）\n",
    "- 最適化しようとしているパフォーマンス指標は精度（`acc`）です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compiling, the model is ready for training.\n",
    "\n",
    "これでトレーニングの準備ができています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The network training / ネットワークのトレーニング\n",
    "\n",
    "The `Model` object provides the `fit` method to handle the training.\n",
    "<br>\n",
    "A basic use of `fit` takes the following parameters:\n",
    "- a set of features: `X_train`\n",
    "- the corresponding labels: `y_train_one_hot` \n",
    "- a batch size: `batch_size`\n",
    "- a number of epochs: `epochs`\n",
    "- a validation set: `validation_data` \n",
    "\n",
    "`Model`オブジェクトはトレーニングを行う` fit`メソッドを提供します。\n",
    "<br>\n",
    "`fit`の基本的な使い方には以下のパラメータが必要です：\n",
    " - 入力データ（特徴）： `X_train`\n",
    " - 対応するラベル： `y_train_one_hot`\n",
    " - バッチの大きさ： `batch_size`\n",
    " - エポックの数： `epochs`\n",
    " - 検証（テスト）データセット： `validation_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`batch_size` and `epochs` control how the optimizer handles the samples:\n",
    "- The batch size defines the number of consecutive samples that are used to estimate the cost function and update the weights.\n",
    "- The optimizer has performed one epoch when it has processed all the samples (it went hrough the dataset one time).\n",
    "\n",
    "For example, if the training set has 35000 samples, the batch size is 100 and `epochs` is set to 20, then the optimizer goes throught the whole `35000` samples `20` times and each time updates the weights `350` times using loss values computed from batches of `100` samples.\n",
    "\n",
    "`batch_size`と` epochs`はオプティマイザがサンプルをどのように扱うかを制御します。\n",
    " - バッチサイズは、コスト関数の推定と重みの更新に使用される連続サンプル数を定義します。\n",
    " - １エポックというのは、オプティマイザがすべてのサンプルの処理を1回実行しました（データセットを1回通過しました）。\n",
    "\n",
    "例えば、トレーニングセットが35000サンプルを有する場合、バッチサイズは100であり、`epochs`は20に設定されたにしましょう。オプティマイザは35000サンプル全体を20回通過します。それぞれのエポックに350回重みを更新します。それぞれの更新には100サンプルのバッチから計算された損失値を使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optional parameter `validation_data` is a dataset composed of features and associated labels.\n",
    "<br>\n",
    "At the end of each epoch the loss function and the accuracy is computed for the validation set.\n",
    "<br>\n",
    "Namely, if we provide the testing set as `validation_data`, the performance on the testing set will be estimated after each epoch.\n",
    "<br>\n",
    "To do that, we need to encode the test set labels as one-hot vectors:\n",
    "\n",
    "オプションのパラメータ `validation_data`は入力データ（特徴）とそれに関連するラベルからなるデータセットです。\n",
    "<br>\n",
    "各エポックの終わりに、検証セット上の損失関数と精度が計算されます。テストセットを`validation_data`として提供すれば、テストセットのパフォーマンスは各エポックの後に推定されます。そのためには、テストセットのラベルに予めワンホットエンコードを実行する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us do the actual training of the network. We run the `fit` method to train `network`. The training takes some time so be patient. \n",
    "\n",
    "それでは、実際にトレーニングを行いましょう。`fit`メソッドを実行して`network`を訓練します。\n",
    "訓練には時間がかかりますのでしばらくお待ちください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "H = network.fit(X_train, y_train_one_hot, batch_size=100, epochs=20, validation_data=(X_test, y_test_one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` method reports for each epoch:\n",
    "- the total computation time of the epoch (rounded)\n",
    "- the computation time per samples\n",
    "- the value of the loss function (on the training set)\n",
    "- the accuracy of the network (on the training set)\n",
    "- the value of the loss function (on the validation set)\n",
    "- the accuracy of the network (on the validation set)\n",
    "\n",
    "各エポックについて`fit`メソッドが以下を報告します：\n",
    " - エポックの合計計算時間（四捨五入）\n",
    " - サンプルあたりの計算時間\n",
    " - （トレーニングセット上の）損失関数の値\n",
    " - （トレーニングセット上の）精度\n",
    " - （テストセット上の）損失関数の値\n",
    " - （テストセット上の）精度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the loss function is the important parameter here.\n",
    "<br>\n",
    "If the training is going well, the loss function should decrease.\n",
    "\n",
    "ここで一番重要なパラメータは損失関数の値です。\n",
    "<br>\n",
    "訓練がうまく行けば、損失関数は減少するはずです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the `fit` method (which we assign to the object `H`) contains a detailed report of the training process.\n",
    "<br>\n",
    "In particular `H.history` is a python dictionary containing the evolution of the loss function and accuracy. \n",
    "<br>\n",
    "It is possible to plot the evolution of the loss function and the accuracy.\n",
    "\n",
    "`fit`メソッドの出力はトレーニング過程の詳細なレポートを含みます。（上のコーデセルではこれを`H`に入れています）\n",
    "<br>\n",
    "`H.history`には損失関数と正確さの進化が入っていますので、損失関数や精度の進化をプロットすることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first look at the loss function:\n",
    "\n",
    "まず損失関数を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(H.history['loss'], 'o-',label=\"loss\")\n",
    "plt.plot(H.history['val_loss'], 'o-', label=\"val_loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"loss vs epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting\n",
    "\n",
    "We can see that the loss function for the training set (blue line) is steadily decreasing whereas the loss function for the testing set (orange line) reaches a minimum and then stays the same or increases a bit.\n",
    "This is a sign of __overfitting__.\n",
    "<br>\n",
    "Overfitting occurs when the classifier starts to learn the specific peculiarities of the samples in the training set. However, by doing that it does not fit well to other data anymore. \n",
    "<br>\n",
    "After the tenth epoch, the network is still learning to better fit the training set. But the featues that are being learned are specific only to the training set and are thus not useful for classifying the testing set.\n",
    "\n",
    "トレーニングセットの損失関数（青い線）は着実に減少していますが、テストセットの損失関数（オレンジ）は最小値に達してから、同じままか少し増加してことがわかります。これは__過学習__の証拠です。\n",
    "<br>\n",
    "過学習は、分類器がトレーニングセット内のサンプルの特定の特性をさらに学習するが、他のデータには適合しなくなる現象です。\n",
    "<br>\n",
    "10回目のエポックの後、ネットワークはまだトレーニングセットによりよく適合するように学習しています。 ただし、学習される機能はトレーニングセットにのみ固有であるため、テストセットの分類には役立ちません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the accuracy:\n",
    "\n",
    "精度もプロットしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(H.history['acc'], 'o-', label=\"acc\")\n",
    "plt.plot(H.history['val_acc'], 'o-', label=\"val_acc\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.title(\"Accuracy vs epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that the accuracy is no longer improving after the tenth epoch.\n",
    "\n",
    "10回目のエポック以降、精度もほとんど向上しなくなったことがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preventing overfitting / 過学習の防止\n",
    "\n",
    "One simple method to prevent overfitting is to _stop the training early_. From the curves above, it seems it would be better to stop the training after 10 epochs. \n",
    "<br>\n",
    "The `fit` method provide a convenient way to do that; it accepts `callbacks`.\n",
    "`callbacks` are functions that are called at specific timings during the training.\n",
    "<br>\n",
    "`keras` defines a callback called `EarlyStopping` that does just what we want - it can stop the training.\n",
    "\n",
    "過学習を防ぐため、1つの簡単な方法は _訓練の中断_ です。例えば、上の曲線を見れば、10エポックの後にトレーニングを中止したら良かったと思われます。\n",
    "<br>\n",
    "`fit`メソッドには`コールバック`関数が使えます。（「コールバック」というのは実行中の特定のタイミングで呼び出される関数です。）\n",
    "<br>\n",
    "訓練を止めるには`EarlyStopping`というコールバックが使えます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the callback for stopping early.\n",
    "\n",
    "トレーニングを早めに中断するためのコールバックを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping_cb = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=2, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of each epoch, this callback checks if the loss function for the validation set has decreased of at least `min_delta`.\n",
    "<br>\n",
    "If for `patience` times it has not decreased, the training is interrupted.\n",
    "\n",
    "各エポックの終わりに、このコールバックはテストセット上の損失関数が`min_delta`以上減少したかどうかチェックします。\n",
    "<br>\n",
    "連続的に減少していない回数が`patience`以上になりますと、訓練は中断されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving best models / 最高のモデルの保存\n",
    "\n",
    "Another useful callback is `ModelCheckpoint` that saves the model with the best performance (on the validation data) to a file.\n",
    "\n",
    "もう1つの便利なコールバックは `ModelCheckpoint`です。これは（テストデータに対して）最高のパフォーマンスのモデルをファイルに保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_checkpoint_cb = ModelCheckpoint(\"best_network.hdf5\", monitor='val_loss', verbose=1, \n",
    "                                      save_best_only=True, save_weights_only=False, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback monitors `val_loss` and saves only the best model (model structure and trained weights) in the file `best_network.hdf5`.\n",
    "\n",
    "このコールバックは`val_loss`を監視し、その値が最低になったモデル（ネットワークの構造と重み）をファイル` best_network.hdf5`に保存します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we just have to provide the callback objects to the `fit` method.\n",
    "<br>\n",
    "The function `fit` expects a list of callbacks. \n",
    "Here we provide a list of two elements.\n",
    "\n",
    "それから、コールバックオブジェクトを `fit`メソッドに渡すだけです。\n",
    "<br>\n",
    "`fit`はコールバックのリストを期待します。ここでは2つの要素のリストを提供します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "H = network.fit(X_train, y_train_one_hot, batch_size=100, epochs=20, validation_data=(X_test, y_test_one_hot),\n",
    "                callbacks=[early_stopping_cb, model_checkpoint_cb], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that `fit` reports saving the model and stops before completing all the epochs.\n",
    "\n",
    "`fit`はモデルの保存を報告し、すべてのエポックを完了する前に停止することがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Reloading the model / 保存したモデルの再読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ModelCheckpoint` callback saved the network with the lowest `val_loss` in the file `best_network.hdf5`.\n",
    "<br>\n",
    "It is possible to load the weights of that `model` with the following command.\n",
    "\n",
    "`ModelCheckpoint`コールバックはファイル` best_network.hdf5`の最も低い `val_loss`でネットワークを保存しました。\n",
    "<br>\n",
    "次のコマンドでその`model`の重みをロードすることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network.load_weights('best_network.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the weights in `network` are set to the weights that were saved in `best_network.hdf5`.\n",
    "\n",
    "これで`network`の重みは`best_network.hdf5`に保存された重みに設定されました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to create a new `Model` using the function `load_model` from the package `tensorflow.keras.models`.\n",
    "\n",
    "パッケージ`tensorflow.keras.models`にある`load_model`関数を使えば新しい`Model`を作成することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "best_network = load_model('best_network.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this command `best_network` is a newly created `Model` identical to the best `Model` that was created during training.\n",
    "\n",
    "このコマンドの後、`best_network`はトレーニング中に作成された最高の`Model`と同一の`Model`です（構造も重みも同じです）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training, the `ModelCheckpoint` callback saved the network.\n",
    "We can also save a `Model` directly by using the `save` method.\n",
    "\n",
    "トレーニング中、`ModelCheckpoint`コールバックはネットワークを保存しました。\n",
    "`save`メソッドを使えば`Model`を直接保存することも可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network.save('my_network.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Performance of the network / ネットワークの性能\n",
    "\n",
    "The `predict` method of a `Model` gives the predicted output for the given input.\n",
    "\n",
    "`Model`の` predict`メソッドは与えられた入力に対する出力の予測を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_one_hot = best_network.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network prediction is the values of the output.\n",
    "<br>\n",
    "Here, it is a vector of length dimension `10`.\n",
    "\n",
    "ネットワーク予測は、出力の値です。\n",
    "<br>\n",
    "ここでは、長さ次元`10`のベクトルです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_one_hot[0,:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the `softmax` activation function in the output layer of the network. The result shows it is dominated by a single value.\n",
    "\n",
    "ネットワークの出力層で`softmax`活性化関数を使いました。予測結果を見ると、多くの場合一つの値だけが大きくなっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.bar(np.arange(10), y_pred_one_hot[0,:])\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"value\")\n",
    "plt.title(\"Network Output\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we display a few samples as an image, we can see that most of the rows are dominated by a single value.\n",
    "\n",
    "いくつかのサンプルの予測結果を画像として表示すると、ほとんどの行では一つの値だけが大きいことがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(y_pred_one_hot[:20,:])\n",
    "plt.xlabel(\"output vector\")\n",
    "plt.xticks(np.arange(10))\n",
    "plt.ylabel(\"samples\")\n",
    "plt.title(\"predictions\")\n",
    "cbar = plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `argmax` from `numpy` gives the index corresponding to the maximum of a numpy array.\n",
    "\n",
    "`numpy`の`argmax`関数は、numpy配列の最大値に対応するインデックスを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"index of the max:\", np.argmax(y_pred_one_hot[0,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By specifying the axis along which to find the maxima (here `axis=1`), it can be applied to the complete prediction block.\n",
    "\n",
    "検索軸を指定すれば（ここでは `axis = 1`）、複数の予測結果の最大値を計算することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `y_pred` contains values in `[0,9]`. These are the predicted values for the digits in the input image.\n",
    "\n",
    "`y_pred`には0から9の値が入っています。入力画像の数字の予測です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y_pred[:20], 's')\n",
    "plt.xlabel(\"samples\")\n",
    "plt.ylabel(\"predicitions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to use the `sklearn` functions to check the performance, like we did last week.\n",
    "\n",
    "先週やっていたように、`sklearn`の数を使ってパフォーマンスを確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "CM = confusion_matrix(y_test, y_pred)\n",
    "print(CM)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(CM)\n",
    "plt.xlabel(\"Predicted digit\")\n",
    "plt.xticks(np.arange(10))\n",
    "plt.yticks(np.arange(10))\n",
    "plt.ylabel(\"True digit\")\n",
    "plt.title(\"True vs predicted digits\")\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_title(\"#samples\", rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try  it yourself ! / 自分で試そう！\n",
    "\n",
    "Since the network created here is taking GPU resources, you should first free them to create networks in other Jupyter notebooks.\n",
    "<br>\n",
    "One easy way to do this is by shuting down or restaring the kernel in this notebook.\n",
    "<br>\n",
    "Run now the menu command `Kernel`->`Restart`.\n",
    "\n",
    "ここで作成されたニューラルネットワークはGPUリソースを使用しています。他のJupyterノートブックでネットワークを作成するためには、まずGPUリソースを解放する必要があります。\n",
    "<br>\n",
    "これを行う簡単な方法の1つは、このノートブックでカーネルを停止または再起動することです。\n",
    "<br>\n",
    "今すぐメニューコマンド`Kernel`->`Restart`を実行してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, [click here](session10-playground.ipynb) to open a sample notebook.\n",
    "\n",
    "次に、[ここをクリックして](session10-playground.ipynb)、サンプルのノートブックを開きます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Should you use neural networks or something else? / ニューラルネットワークを使うべきか、それとも他のものを使うべきか？\n",
    "\n",
    "There is no easy answer to what machine learning method to use. Here are a few hints.\n",
    "\n",
    "- For very large datasets and complex tasks (e.g. determining what is in an image), (deep) neural networks will nowadays typically work best. Although, determining which network structure to use (how many layers etc.) can be difficult.\n",
    "\n",
    "- For smaller datasets you can achieve good results with simpler methods. Two methods which usually perform well are (nonlinear) SVM and Random Forests. Random Forests also have very little parameters so it is perhaps one of the simplest methods to try (see sklearn.ensemble.RandomForestClassifier).\n",
    "\n",
    "- If the dataset is small, training a large neural network does not make sense (in some cases transfer learning can be used, see session 12). You can train a small neural network with a few layers, but you should also try other methods like SVM as they might work better.\n",
    "\n",
    "- Sometimes it may be enough for you to use something even simpler, like Perceptron or Logistic Regression (for example, you might not need to use a complex method to achieve 99% accuracy if a simple one can give you 97%).\n",
    "\n",
    "- In general, you should try comparing multiple methods, starting with simple ones, until you achieve the desired performance.\n",
    "\n",
    "\n",
    "どの機械学習手法を使うべきか、簡単な答えはありません。ここではいくつかのヒントを紹介します。\n",
    "\n",
    "- 非常に大きなデータセットや複雑なタスク（例えば、画像に何が写っているかを判断する）には、最近では、（ディープ）ニューラルネットワークが最も効果的です。しかし、どのようなネットワーク構造を使用するか（何層にするかなど）を決定するのは難しいかもしれません。\n",
    "\n",
    "- データセットが小さい場合は、よりシンプルな手法でも良い結果が得られます。通常、良い結果が得られる2つの手法は、（非線形）SVMとランダムフォレストです。ランダムフォレストもパラメータが少ないので、使いやすい手法の1つです（sklearn.ensemble.RandomForestClassifierを参照）。\n",
    "\n",
    "- データセットが小さい場合、大規模なニューラルネットワークを学習しても意味がありません（場合によっては転移学習が使えます、セッション12参照）。いくつかの層を持つ小さなニューラルネットワークを学習することができます。ただ、その際、SVMなどの手法の結果と比較したほうがいいです。\n",
    "\n",
    "- 場合によっては、パーセプトロンやロジスティック回帰のようなもっと単純な方法で十分なこともあります（例えば、単純な方法で97％の精度が得られるなら、99％の精度を得るために複雑な方法を使う必要はないかもしれません）。\n",
    "\n",
    "- 一般的には，目的の性能が得られるまで，単純なものから始めて，複数の手法を比較してみるとよいでしょう。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
