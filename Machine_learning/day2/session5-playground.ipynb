{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session5 - playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the necessary packages\n",
    "- Load the iris dataset\n",
    "- Create the observations `X` and labels `y`\n",
    "\n",
    "\n",
    "- 必要なパッケージをインポートします\n",
    "- アヤメのデータセットを読み込みます\n",
    "- 観測値`X`とラベル`y`を作成します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X has shape: (150, 4)\n",
      "y has shape: (150,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "iris = np.genfromtxt('iris_data.csv', delimiter=',')\n",
    "\n",
    "X = iris[:,1:]\n",
    "y = iris[:,0]\n",
    "print(\"X has shape:\", X.shape)\n",
    "print(\"y has shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: preparation\n",
    "\n",
    "Copy your code for data preparation from day1, session4-playground1 (both TASK1 and TASK2) in the cell below and execute it.\n",
    "\n",
    "以下のセルにday1のsession4-playground1からデータ準備用のコード（TASK1とTASK2の両方）をコピーして、実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.4)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train);\n",
    "\n",
    "X_train_raw = X_train\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_test_raw = X_test\n",
    "X_test = scaler.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2\n",
    "- Create and train a linear support vector classifier\n",
    "\n",
    "\n",
    "- 線形サポートベクトル分類器を作成し、トレーニングさせる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "linear_SVC = LinearSVC()\n",
    "linear_SVC.fit(X_train, y_train); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3\n",
    "- Predict the labels for the test features\n",
    "\n",
    "\n",
    "- テストデータのラベルを予測する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linear_SVC.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 4\n",
    "- Show the confusion matrix\n",
    "- Compute the accuracy (you should obtain more than 0.9 (90%) accuracy)\n",
    "\n",
    "\n",
    "- 混同行列を表示する\n",
    "- 精度を計算する（0.9（90％）以上の精度が得られるはずです）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0]\n",
      " [ 0 21  0]\n",
      " [ 0  1 22]]\n",
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "CM = confusion_matrix(y_test, y_pred)\n",
    "print(CM)\n",
    "\n",
    "A = np.sum(np.diag(CM)) / np.sum(CM)\n",
    "print(\"Accuracy: {:.02f}\".format(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL 1\n",
    "- Run the training another time and compare (tip: you can use `Cell -> Run All` from the menu)\n",
    "- Change the proportion of test samples and see the impact on the performance\n",
    "\n",
    "\n",
    "- トレーニングをもう一度実行して、結果を比較します（ヒント：メニューから`Cell -> Run All`を利用できます）\n",
    "- テストサンプルの比率を変更し、パフォーマンスへの影響を確認する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果\n",
    "- トレーニングの回数を増やすと精度が向上した\n",
    "    - 1回目: 0.92\n",
    "    - 2回目: 0.96\n",
    "  \n",
    "- テストサンプルの比率を減らすと精度が向上した\n",
    "    - テストサイズ0.5: 0.92\n",
    "    - テストサイズ0.4: 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL 2\n",
    "\n",
    "The splitting between training and testing set does not care if there is an \"equal\" number of each flower type in the sets.\n",
    "\n",
    "- Write some code to split the dataset randomly and care about having an equal representation of each flower type (you can start from the code for manually spliting in the main notebook)\n",
    "- Test the average performance of the classifier using this dataset splitting method\n",
    "\n",
    "トレーニングセットとテストセットの分割は、セット内の各花の種類が「等しい」数であるかどうかを考慮していません。\n",
    "\n",
    "- データセットをランダムに分割し、各花の種類の数が同じになるためのコードを実装してください（ヒント：メインのノートブックにある手動の分割のコードを元に開発できます）\n",
    "\n",
    "- このデータセット分割方法を使用して、分類器の平均パフォーマンスを確認してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "for i in range(0,3):\n",
    "    # irisを花の種類ごとに前もって分割\n",
    "    iris_splitted = iris[iris[:,0] == i, :]\n",
    "    X_splitted = iris_splitted[:,1:]\n",
    "    y_splitted = iris_splitted[:,0]\n",
    "    # それぞれをランダムに分割\n",
    "    X_train_splitted, X_test_splitted, y_train_splitted, y_test_splitted = train_test_split(X_splitted, y_splitted, test_size=0.5)\n",
    "    # 統合\n",
    "    if i == 0:\n",
    "        X_train = X_train_splitted\n",
    "        X_test = X_test_splitted\n",
    "        y_train = y_train_splitted\n",
    "        y_test = y_test_splitted\n",
    "    else:\n",
    "        X_train = np.concatenate([X_train, X_train_splitted])\n",
    "        X_test = np.concatenate([X_test, X_test_splitted])\n",
    "        y_train = np.concatenate([y_train, y_train_splitted])\n",
    "        y_test = np.concatenate([y_test, y_test_splitted])\n",
    "\n",
    "# あとはいつもどおり\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train);\n",
    "\n",
    "X_train_raw = X_train\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_test_raw = X_test\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "\n",
    "linear_SVC = LinearSVC()\n",
    "linear_SVC.fit(X_train, y_train); \n",
    "y_pred = linear_SVC.predict(X_test)\n",
    "\n",
    "\n",
    "CM = confusion_matrix(y_test, y_pred)\n",
    "A = np.sum(np.diag(CM)) / np.sum(CM)\n",
    "print(\"Accuracy: {:.02f}\".format(A))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
