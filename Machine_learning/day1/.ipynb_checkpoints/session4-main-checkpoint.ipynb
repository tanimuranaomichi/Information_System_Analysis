{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing / データの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__scikit-learn__ (`sklearn`) is a Python package which contains a large number of functions for doing machine learning, including data processing and classification.\n",
    "<br>\n",
    "It contains many subpackages. In the code below we will import the necessary subpackages when they are needed.\n",
    "\n",
    "__scikit-learn__（`sklearn`）は、前処理、分類など機械学習を実行するための多数の関数を含むPythonパッケージです。\n",
    "<br>\n",
    "多くのサブパッケージが含まれています。以下のコードでは、必要なサブパッケージを適宜にインポートします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset / アヤメのデータセット\n",
    "\n",
    "### Observations and labels / 観測とラベル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the \"iris dataset\" with 150 records and five attributes: flower type, sepal length, sepal width, petal length, petal width.\n",
    "\n",
    "150のサンプルと5つの属性（花の種類、がくの長さ、がくの幅、花びらの長さ、花びらの幅）を持つ「アヤメのデータセット」を使用します。\n",
    "<br>\n",
    "<img src=\"./img/petal-sepal.jpg\" width=\"150\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = np.genfromtxt('iris_data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample is composed of:\n",
    "- A flower type 0 (iris setosa), 1 (iris versicolor) or 2 (iris virginica) ⇒ labels\n",
    "- The 4 sepal and petal measurements ⇒ observations\n",
    "\n",
    "各サンプルは次のもので構成されています。\n",
    "- 花の種類：0（ヒオウギアヤメ）、1（ブルーフラッグ）または 2（バージニカ） ⇒ ラベル\n",
    "- 花びらとがくの4つの測定データ ⇒ 観察"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the remainder, we will use the letters `X` for the observations and `y` for the labels.\n",
    "\n",
    "これ以降は、観測データには文字`X`を使用し、ラベルには`y`を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X has shape: (150, 4)\n",
      "y has shape: (150,)\n"
     ]
    }
   ],
   "source": [
    "X = iris[:,1:]\n",
    "y = iris[:,0]\n",
    "print(\"X has shape:\", X.shape)\n",
    "print(\"y has shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation / データの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting into training and testing set / トレーニングとテストセットに分割\n",
    "\n",
    "Most times a classifier is trained in order to correctly predict labels for _new future observations_.\n",
    "<br>\n",
    "Consequently, what really matters is the performance of the classifier when presented with new obervations.\n",
    "<br>\n",
    "A practical way to measure future performance is to split the available examples in two sets:\n",
    "- A set that is used for training ⇒ __training set__\n",
    "- A set that is used for testing the performance ⇒ __testing set__\n",
    "\n",
    "\n",
    "ほとんどの場合、分類器をトレーニングさせるり用は_新しい将来の観測_のラベルを正しく予測するためです。\n",
    "<br>\n",
    "そのため、本当に重要なのは、新しい観測を提示したときの分類器のパフォーマンスです。\n",
    "<br>\n",
    "将来のパフォーマンスを測定する実際的な方法は、使用可能な例を2つのセットに分割することです。\n",
    "- トレーニングに使用されるセット ⇒ __トレーニングセット__\n",
    "- パフォーマンスのテストに使用されるセット ⇒ __テストセット__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples in the testing set are playing the role of the \"never seen before\" observations.\n",
    "<br>\n",
    "Computing the accuracy on the testing set gives us an idea of how well the trained classifier __generalizes__, i.e. how well it classifies observations that are not part of the training set.\n",
    "\n",
    "テストセットは、「これまでに見たことのない」観測の役割を果たしています。\n",
    "<br>\n",
    "テストセットの精度を計算すると、トレーニングされた分類器がどの程度__一般化__するか（つまり、トレーニングセットに入ってない観測をどの程度適切に分類でいるか）がわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us divide the dataset into a __training__ and a __testing__ set.\n",
    "<br>\n",
    "The samples will be _randomly_ associated to the training or the testing set.\n",
    "\n",
    "まず、データセットを__トレーニングセット__と__テストセット__に分割しましょう。\n",
    "<br>\n",
    "サンプルは、トレーニングまたはテストセットに_ランダムに_割り当てられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_▶ Using sklearn to split data / sklearnを使用したデータの分割_\n",
    "\n",
    "We use the function `train_test_split` from `sklearn.model_selection` to split the data into training and testing sets.\n",
    "\n",
    "`sklearn.model_selection`の`train_test_split`関数を使用して、データをトレーニングセットとテストセットに分割できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 105\n",
      "Testing set size: 45\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# test_size=0.3 ⇒ 30％ of data in test set, the rest (70%) in training set\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Testing set size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many flowers of each species are in the training set and the testing set.\n",
    "\n",
    "トレーニングセットとテストセットに含まれる各種の花の数を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set:\n",
      "Iris setosa: 32\n",
      "Iris versicolor: 38\n",
      "Iris virginica: 35\n"
     ]
    }
   ],
   "source": [
    "print(\"training set:\")\n",
    "print(\"Iris setosa:\", np.sum(y_train == 0))\n",
    "print(\"Iris versicolor:\", np.sum(y_train == 1))\n",
    "print(\"Iris virginica:\", np.sum(y_train == 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing set:\n",
      "Iris setosa: 18\n",
      "Iris versicolor: 12\n",
      "Iris virginica: 15\n"
     ]
    }
   ],
   "source": [
    "print(\"testing set:\")\n",
    "print(\"Iris setosa:\", np.sum(y_test == 0))\n",
    "print(\"Iris versicolor:\", np.sum(y_test == 1))\n",
    "print(\"Iris virginica:\", np.sum(y_test == 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_▶ Manually splitting data / 手動のデータ分割 (optional)_\n",
    "\n",
    "For reference, here is the code for splitting the data manually. It does the same thing as `train_test_split`.\n",
    "<br>\n",
    "(If you want to run the code, first transform the cell into a code cell.)\n",
    "\n",
    "参考までに、データを手動で分割するコードを次に示します。`train_test_split`と同じことを行います。\n",
    "<br>\n",
    "（コードを実行したい場合は、まずセルをコードセルに変換してください。）"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Create a permutation of the indicies\n",
    "number_samples = X.shape[0]\n",
    "shuffle_index = np.random.permutation(number_samples)\n",
    "\n",
    "# use the permuted list as indices\n",
    "X = X[shuffle_index]\n",
    "y = y[shuffle_index]\n",
    "\n",
    "# Split the data in training and testing\n",
    "testing_ratio = 0.3 # 30%\n",
    "test_samples = int(testing_ratio * number_samples)\n",
    "\n",
    "# from 0 to test_samples-1\n",
    "X_test = X[:test_samples]\n",
    "y_test = y[:test_samples]\n",
    "\n",
    "# From test_samples to end\n",
    "X_train = X[test_samples:]\n",
    "y_train = y[test_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data standardization / データの標準化\n",
    "\n",
    "The goal of standardization is to transform the features so that they have zero mean and unit variance. This is important to do to prevent that some features have larger influence on the result than others. \n",
    "\n",
    "標準化の目的は、平均と単位分散がゼロになるように特徴を変換することです。これは、一部の特徴が他の特徴よりも結果に大きな影響を与えることを防ぐために行うことが重要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package `sklearn.preprocessing` provides a `StandardScaler` that computes the scaling parameters from a training set and makes it possible to easily apply these parameters to new data. This is the function we will usually use for scaling.\n",
    "\n",
    "パッケージ`sklearn.preprocessing`は、トレーニングセットからスケーリングパラメーターを計算し、これらのパラメーターを新しいデータに適用できるようにする`StandardScaler`を提供します。標準化を行うために使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the parameters of the `StandardScaler` have to be computed from the training set. This is done with the method `fit`.\n",
    "\n",
    "まず、`StandardScaler`のパラメーターをトレーニングセットから計算する必要があります。これはメソッド`fit`で行われます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can create a scaled training set using the method `transform`.\n",
    "\n",
    "次に、`transform`メソッドを使用して、スケーリングされたトレーニングセットを作成できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = X_train # We keep a copy of the original training set for plotting\n",
    "X_train = scaler.transform(X_train_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the scaled data to see how it looks like.\n",
    "\n",
    "プロットして、スケーリングされたデータがどのように見えるか見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train_raw[:,2], 'b-', label=\"Sepal length\")\n",
    "plt.plot(X_train[:,2], 'b--', label=\"Sepal length scaled\")\n",
    "plt.plot(X_train_raw[:,3], 'r-', label=\"Sepal width\")\n",
    "plt.plot(X_train[:,3], 'r--', label=\"Sepal width scaled\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to scale also the observations in the test set.\n",
    "\n",
    "テストセットの観測値もスケーリングする必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_raw = X_test\n",
    "X_test = scaler.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> IMPORTANT: Fit the scaler to the _training data only_, not to the full dataset (including the test set). Only then use it to transform both the training set and the test set.\n",
    "\n",
    ">重要：fit関数は全部のデータセットではなく、__トレーニングデータのみ__に適合させます。それを使用してトレーニングセットとテストセットの両方を変換します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try  it yourself ! / 自分で試そう！\n",
    "\n",
    "[Click here](session4-playground.ipynb) to open a sample notebook and try doing data processing\n",
    "\n",
    "[ここをクリックして](session4-playground.ipynb)、サンプルのノートブックを開き、データ処理を行ってみてください\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
